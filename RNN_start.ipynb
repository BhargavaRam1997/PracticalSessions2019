{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EEML2019_RNN_start.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfIdnkh92358",
        "colab_type": "text"
      },
      "source": [
        "# RNN training tutorial\n",
        "\n",
        "This is a tutorial training various RNNs on simple datasets and doing some analysis.\n",
        "\n",
        "Structure:\n",
        "\n",
        "  1. basic (vanilla RNN) implementation\n",
        "  2. observing exploding/vanishing gradients\n",
        "  3. training an LSTM on character level langugage modelling task\n",
        "    * comparing training of an LSTM and RNN, playing with architectures\n",
        "  4. Intepretability by plotting and analysing activations of a network:\n",
        "    * identifying interpretable neurons\n",
        "    * identifying neurons-gates interactions\n",
        "    * identifying hidden state dynamics through time\n",
        "  \n",
        "    \n",
        "\n",
        "First three sections are almost independent, one can go switch between them without any code dependencies (apart from being unable to use vanilla RNN in section 4, if it was not implemented in 1.).\n",
        "\n",
        "Cells that include \"starting point\" in their title require filling in some code gaps; all remaining ones are complete (but feel free to play with them if you want!)\n",
        "\n",
        "Please pay attention to questions after each section. Finding out answers to these is crucial to make sure one understands various modes of RNN operation.\n",
        "\n",
        "Language model exercises are based on [Sonnet LSTM example](https://github.com/deepmind/sonnet/blob/master/sonnet/examples/rnn_shakespeare.py).\n",
        "Apart from loading the dataset, we make no further use of Sonnet in this colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA_K3_OL3EY4",
        "colab_type": "text"
      },
      "source": [
        "## Imports\n",
        "\n",
        "We will use tf.nn.rnn_cell and tf.layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5YGV2hb2RIt",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.nn.rnn_cell as rnn_cell\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sonnet.examples import dataset_shakespeare\n",
        "  \n",
        "sns.set_style('ticks')  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0KPZdiq5AVJ",
        "colab_type": "text"
      },
      "source": [
        "# Ex 1.    Vanilla RNN\n",
        "\n",
        "Implement basic RNN cell using tf.layers.\n",
        "\n",
        "   $$ h_t = f( Wx_t + Vh_{t-1}  + b) $$\n",
        "   \n",
        "   Where\n",
        "   \n",
        "   * $x_t$ input at time $t$\n",
        "   * $h_t$ hidden state at time $t$\n",
        "   * $W$ input-to-hidden mapping (trainable)\n",
        "   * $V$ hidden-to-hidden mapping (trainable)\n",
        "   * $b$ bias (trainable)\n",
        "   * $f$ non-linearity chosen (usually tanh)\n",
        "   \n",
        "   \n",
        "   You do not need to worry about the plotting and running code, but focus on the RNN implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuLRYckbfAuf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hint: 'tf.layers.Dense' implements simple linear layers\n",
        "\n",
        "tf.layers.Dense?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6GIHgOnzd8Y",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Vanilla RNN - Solution\n",
        "class RNN(rnn_cell.RNNCell):\n",
        "  \n",
        "  def __init__(self, hidden_size, activation=tf.tanh, name=\"vanilla_rnn\"):    \n",
        "    \"\"\"\n",
        "    Args:\n",
        "    \n",
        "      hidden_size: number of hidden units\n",
        "      activation: function constructing activation op\n",
        "      name: name of the core\n",
        "    \"\"\"    \n",
        "    # We have to call parent's constructor with the name provided\n",
        "    super(RNN, self).__init__(name=name)\n",
        "\n",
        "    # Save these arguments for later\n",
        "    self._hidden_size = hidden_size  # Must be an integer\n",
        "    self._activation = activation  # Notice this function will construct an op.\n",
        "\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    \"\"\"\n",
        "    This function will is typically used to create the variables of the model.\n",
        "    \n",
        "    Args:\n",
        "  \n",
        "      input_shape: tensor shape of the input\n",
        "    \"\"\"\n",
        "    \n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "    # Use a linear layer to implement the trainable input to hidden\n",
        "    # mapping \"W\" and the trainable bias \"b\", see hint above\n",
        "    # Please note that such a layer *will* also implement the bias by default\n",
        "    # Please don't connect it to anything yet\n",
        "    self._in_to_hidden_linear = None\n",
        "\n",
        "\n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "    # Use another linear layer to implement the trainable\n",
        "    # hidden to hidden mapping \"V\"\n",
        "    # Please note that this second layer does not require a bias,\n",
        "    # since the first layer already implements it\n",
        "    # Please don't connect it to anything yet\n",
        "    self._hidden_to_hidden_linear = None\n",
        "\n",
        "    # Checks\n",
        "    msg = (\"Implement a linear 'Dense' mapping from input to hidden \"\n",
        "           \"with 'self._hidden_size' units\")\n",
        "    assert self._in_to_hidden_linear is not None, msg\n",
        "\n",
        "    msg = \"Implement a linear 'Dense' layer \"\n",
        "    assert self._hidden_to_hidden_linear is not None, msg\n",
        "\n",
        "\n",
        "  def call(self, input_, prev_state):\n",
        "    \"\"\"\n",
        "    This function will be called in a loop, when RNN core is connected to\n",
        "    ops creating inputs and previous states.\n",
        "    \n",
        "    Args:\n",
        "    \n",
        "      input_: tensor containing current x_t\n",
        "      prev_state: tensor containing previous state, h_{t-1}\n",
        "    \"\"\"\n",
        "\n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "    # Compute the output of the RNN Cell by implementing the equation above.\n",
        "    output = None\n",
        "\n",
        "    # Checks\n",
        "    assert output is not None, \"The RNN must compute the output\"\n",
        "\n",
        "    # Cores return pairs of (o_t, h_t) where o_t is the output exposed\n",
        "    # to the rest of the code, for vanilla RNN these are the same quantities\n",
        "    return output, output\n",
        "\n",
        "\n",
        "  @property\n",
        "  def state_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])\n",
        "\n",
        "\n",
        "  @property\n",
        "  def output_size(self):\n",
        "    return tf.TensorShape([self._hidden_size])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jCR9YGaI7my",
        "colab_type": "text"
      },
      "source": [
        "### Train the RNN\n",
        "\n",
        "Train the RNN on sine data - predict the next sine value from *predicted* sine values.\n",
        "\n",
        "Predict   $$ sin (x +t \\epsilon) $$ from $$ sin (x), sin (x + \\epsilon), ..., sin (x + (t-1) \\epsilon) $$\n",
        "\n",
        "In particular, we want the network to predict the next value in a loop, conditioning the prediction on some initial values (provided) and all subsequent predictions.\n",
        "\n",
        "To learn the prediction model, we will use *teacher forcing*. This means that when training the model, the input at time $t$ is the real sequence at time $t$, rather than the output produced by the model at $t-1$. When we want to generate data from the model, we do not have access to the true sequence, so we do not use teacher forcing. However, in the case of our problem, we will also use *warm starting*, because we require multiple time steps to predict the next sine wave value (at least 2, for the initial value and for the step). \n",
        "\n",
        "The code below unrolls the RNN core you have defined above, does the training using backprop though time and plots the real data (\"ground truth\"), the data generated during training (\"train predictions\") and the model samples \"generated\".\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dzlEv0lo7Bh",
        "colab_type": "code",
        "outputId": "f898ea87-e915-4f5a-f936-faa0121661b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Hint:\n",
        "help(rnn_cell.RNNCell.get_initial_state)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on method get_initial_state in module tensorflow.python.ops.rnn_cell_impl:\n",
            "\n",
            "get_initial_state(self, inputs=None, batch_size=None, dtype=None) unbound tensorflow.python.ops.rnn_cell_impl.RNNCell method\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paHrijB1kbiQ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Training a simple sequence, using teacher forcing and sampling\n",
        "\n",
        "UNROLL_LENGTH = 30  #@param {type:\"integer\"}\n",
        "NUM_ITERATIONS = 10000  #@param {type:\"integer\"}\n",
        "WARM_START = 10  #@param {type:\"integer\"}\n",
        "TEACHER_FORCING = False  #@param {type:\"boolean\"}\n",
        "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
        "LEARNING_RATE = 0.0001  #@param {type:\"number\"}\n",
        "REPORTING_INTERVAL = 2000  #@param {type:\"integer\"}\n",
        "\n",
        "# We create training data, sine wave over [0, 2pi]\n",
        "x_train = np.arange(0, 2*np.pi, 0.1).reshape(-1, 1, 1)\n",
        "y_train = np.sin(x_train)\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "# Create an RNN object using the class we defined in the previous cell\n",
        "rnn = None\n",
        "\n",
        "# Checks\n",
        "assert rnn is not None, \"Create an RNN object using the class defined above.\"\n",
        "\n",
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "# Create a linear mapping from RNN output to the target (scalar)\n",
        "# For clarity, name it something along the lines: \"hidden_to_output\"\n",
        "dec = None\n",
        "\n",
        "# Checks\n",
        "assert dec is not None, \"Create RNN output decoder object\"\n",
        "\n",
        "# Placeholder is gonna be used to provide a subsequence on which we will learn.\n",
        "# Each subsequence will be a consecutive block of UNROLL_LENGTH values of\n",
        "# the sine wave\n",
        "sequence = tf.placeholder(tf.float32, [UNROLL_LENGTH, 1, 1])\n",
        "\n",
        "\n",
        "# First, the training section\n",
        "\n",
        "losses = []\n",
        "train_predictions = []\n",
        "\n",
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "# Get the initial state of the RNN with batch_size 1 and dtype tf.float32\n",
        "current_state = None \n",
        "\n",
        "# Checks\n",
        "assert current_state is not None, \"Get the current state with batch_size=1 and type tf.float32.\"\n",
        "\n",
        "\n",
        "# For simplicity, we will unroll our RNN by hand in a loop\n",
        "for i in range(UNROLL_LENGTH-1):\n",
        "  \n",
        "  # In teacher forcing setup, input is the true previous output\n",
        "  if TEACHER_FORCING:\n",
        "    input_ = sequence[i]\n",
        "  else:\n",
        "    # In \"generative\" mode the input is our own previous prediction\n",
        "    if i <= WARM_START:\n",
        "      # We can still use teacher forcing at the very beginning of training\n",
        "      input_ = sequence[i]\n",
        "    else:\n",
        "      input_ = prediction\n",
        "\n",
        "  ######################\n",
        "  #   YOUR CODE HERE   #\n",
        "  ######################\n",
        "  # Connect the RNN: we apply our cell to (input, state) pair, and we get\n",
        "  # (output, next_state) pair in return; current_state becomes next_state.\n",
        "  output, current_state = None, current_state\n",
        "\n",
        "  # Checks\n",
        "  assert output is not None, \"The RNN must return output\"\n",
        "\n",
        "  ######################\n",
        "  #   YOUR CODE HERE   #\n",
        "  ######################\n",
        "  # Connect output decoder to get prediction at this step\n",
        "  prediction = None\n",
        "\n",
        "  assert prediction is not None, \"Connect state decoder to get prediction\"\n",
        "\n",
        "  losses.append(tf.reduce_sum((prediction - sequence[i+1])**2))\n",
        "  train_predictions.append(prediction)\n",
        "\n",
        "loss = tf.add_n(losses) / len(losses)\n",
        "train_predictions = tf.stack(train_predictions)\n",
        "\n",
        "\n",
        "# Now we define a part of the graph used for generating data\n",
        "# The code is almost the same as the training one, with the only difference\n",
        "# being that now we unroll over entire signal, and use our own predictions\n",
        "# to condition ourselves.\n",
        "\n",
        "current_state = rnn.get_initial_state(batch_size=1, dtype=tf.float32)\n",
        "predictions = []\n",
        "sampling_losses = []\n",
        "for i in range(len(y_train)-1):\n",
        "  if i <= WARM_START:\n",
        "    input_ = tf.constant(y_train[i], dtype=tf.float32)\n",
        "  else:\n",
        "    input_ = prediction\n",
        "  output, current_state = rnn(input_, current_state)\n",
        "  prediction = dec(output)\n",
        "  sampling_losses.append(tf.reduce_sum((prediction - y_train[i+1])**2))\n",
        "  predictions.append(prediction)\n",
        "sampling_loss = tf.add_n(sampling_losses) / len(sampling_losses)\n",
        "predictions = tf.stack(predictions)\n",
        "\n",
        "# Checks and training op\n",
        "msg = (\"You should have created 5 tensorflow variables instead of {}.\\n\"\n",
        "       \"Here are all then trainable variables you have created:\\n{}\\n\\n\"\n",
        "       \"Are you sure parameter sharing is working as expected?\\n\\n\"\n",
        "       \"How many bias variables have you created inside the cell?\").format(\n",
        "    len(tf.trainable_variables()),\n",
        "    \"\\n\".join([str(v) for v in tf.trainable_variables()]))\n",
        "assert len(tf.trainable_variables()) == 5, msg\n",
        "\n",
        "opt = tf.train.AdamOptimizer(LEARNING_RATE)\n",
        "train_op = opt.minimize(loss)\n",
        "\n",
        "\n",
        "# At this point entire graph is built, and we can create the session and run\n",
        "\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  for i in range(NUM_ITERATIONS + 1):\n",
        "    start = np.random.choice(\n",
        "      range(x_train.shape[0] - UNROLL_LENGTH)\n",
        "    )\n",
        "    l, p, _ = sess.run(\n",
        "        [loss, train_predictions, train_op], \n",
        "        feed_dict={sequence: y_train[start: start+UNROLL_LENGTH]})\n",
        "    \n",
        "    if i % REPORTING_INTERVAL == 0:\n",
        "      pred, pred_l = sess.run([predictions, sampling_loss])\n",
        "      plt.figure()\n",
        "      plt.title('Training Loss %f;  Sampling loss %f; Iteration %d' % (l, pred_l, i))\n",
        "      \n",
        "      plt.plot(y_train[1:].ravel(), c='blue', label='Ground truth',\n",
        "               linestyle=\":\", lw=6)\n",
        "      plt.plot(range(start, start+UNROLL_LENGTH-1), p.ravel(), c='gold',\n",
        "               label='Train prediction', lw=5, marker=\"o\", markersize=5,\n",
        "               alpha=0.7)\n",
        "      plt.plot(pred.ravel(), c='r', label='Generated', lw=4, alpha=0.7)\n",
        "      plt.legend()\n",
        "      plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXXQSfxREqKf",
        "colab_type": "text"
      },
      "source": [
        "After you've checked for bugs, please add your final sampling losses to this spreadsheet in a new row: https://docs.google.com/spreadsheets/d/1Zpi_A6RP89E00vurqz9dRHYCd29PqzB9VB7Y4giydyA/edit#gid=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLuIAK8LJWay",
        "colab_type": "text"
      },
      "source": [
        "**Note:** initialization is not fixed (we do not fix a random seed), so each time the cell is executed, the parameters take new initial values and hence training can lead to different results. What happens if you run it multiple times?\n",
        "\n",
        "###What is worth trying/understanding here?\n",
        "\n",
        "* Difference between teacher forcing and learning on own samples:\n",
        " * What are the pros and cons of teacher forcing?\n",
        " * Why is the model struggling to learn in one of the setups?\n",
        " * What is it we actually care about for models like this? What should be the actual surrogate?\n",
        "* How does warm starting affect our training? Why?\n",
        "* What happens if the structure of interest is much longer than the unroll length?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7Bj03cut3Ex",
        "colab_type": "text"
      },
      "source": [
        "Answers:\n",
        "* Teacher forcing because BPTT is much easier and works better in practice. Intuition is similar to immitation learning. Without TF it is very hard to learn because error tend to accumulate. If you use TF then you get very local structure.\n",
        "* No teacher forcing makes training very difficult.\n",
        "* Depending on what you want to model, this loss may be fine if you care about probabilities but not generating samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxyUegmC5_Hj",
        "colab_type": "text"
      },
      "source": [
        "# Ex. 2      Vanishing and exploding gradients\n",
        "\n",
        "Given an input sequence $(x_1, ..., x_N)$ of random floats (sampled from normal distribution), train an RNN as before and compute the gradients of the last output state w.r.t. every previous state:\n",
        "$$\n",
        "\\left \\| \\frac{\\partial h_{N}}{\\partial h_i} \\right \\|\n",
        "$$\n",
        "for each unroll $i$, and plot these quantities for various RNNs.\n",
        "\n",
        "Note, that during learning one would compute\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial \\theta}  \n",
        "$$\n",
        "which, using chain rule will involve terms like\n",
        "$$\n",
        "\\frac{\\partial L}{\\partial h_N} \\cdot\n",
        "\\frac{\\partial h_N}{\\partial h_{N-1}} \\cdot\n",
        "\\dots \\cdot\n",
        "\\frac{\\partial h_i}{\\partial h_{i-1}} \\cdot\n",
        "\\dots \\cdot\n",
        "\\frac{\\partial h_0}{\\partial \\theta}\n",
        "$$\n",
        "so if one of them vanishes, all of them do."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULSWaWPtpynM",
        "colab_type": "text"
      },
      "source": [
        "# Hints:\n",
        "\n",
        "Tensorflow already defines many types of RNN Cells, such as LSTM, GRU, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5BNQId1qibA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn_cell.LSTMCell?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAL8UB3QvoF7",
        "colab_type": "text"
      },
      "source": [
        "NB: There is no training here, we are just computing the norms of the gradients of the last hidden state with respect to the hidden state across steps in the sequence.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zljqN01vc9-3",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Vanishing and exploding gradients\n",
        "tf.reset_default_graph()\n",
        "\n",
        "SEQ_LENGTH = 15  #@param {type:\"integer\"}\n",
        "HIDDEN_UNITS = 20  #@param {type:\"integer\"}\n",
        "\n",
        "dummy_input = [tf.constant([[np.random.normal()]]) for _ in range(SEQ_LENGTH)] \n",
        "\n",
        "######################\n",
        "#   YOUR CODE HERE   #\n",
        "######################\n",
        "# Add several cell constructors (use those already defined in Tensorflow) to the\n",
        "# list (e.g., also add a GRU, and a few more LSTMS with their initial \n",
        "# forget_bias values set to: 0, +1, +2 and -2).\n",
        "# If in doubt, check the documentation.\n",
        "rnn_types = {\n",
        "    'RNN': lambda nhid: RNN(nhid),\n",
        "    # Hint:'LSTM': lambda nhid: rnn_cell.XYZ,\n",
        "}\n",
        "\n",
        "# Checks\n",
        "assert len(rnn_types) >= 2, \"Add at least one more RNN cell to the list.\"\n",
        "\n",
        "depths = {rnn_type: [] for rnn_type in rnn_types}\n",
        "grad_norms = {rnn_type: [] for rnn_type in rnn_types}\n",
        "\n",
        "for rnn_type in rnn_types:\n",
        "  \n",
        "  constructor = rnn_types[rnn_type]\n",
        "  \n",
        "  rnn = constructor(HIDDEN_UNITS)\n",
        "\n",
        "  rnn_at_time = []\n",
        "  gradients_at_time = []\n",
        "\n",
        "  prev_state = rnn.get_initial_state(batch_size=1, dtype=tf.float32)\n",
        "  \n",
        "  for i in range(SEQ_LENGTH):\n",
        "    _, prev_state = rnn(\n",
        "      dummy_input[i], prev_state\n",
        "    )\n",
        "    rnn_at_time.append(prev_state)\n",
        "\n",
        "  # We don't really care about the loss here: we are not solving a specific \n",
        "  # problem, any loss will work to inspect the behavior of the gradient.\n",
        "  dummy_loss = tf.reduce_sum(rnn_at_time[-1])\n",
        "  \n",
        "  for i in range(1, SEQ_LENGTH):\n",
        "    current_gradient = tf.gradients(\n",
        "      dummy_loss, \n",
        "      rnn_at_time[i],   \n",
        "    )\n",
        "    gradients_at_time.append(current_gradient)\n",
        "  \n",
        "  init = tf.global_variables_initializer()  \n",
        "  with tf.train.SingularMonitoredSession() as sess:\n",
        "    sess.run(init)\n",
        "    gradients = sess.run(gradients_at_time)\n",
        "\n",
        "  for gid, grad in enumerate(gradients):\n",
        "    depths[rnn_type].append(len(gradients)-gid)    \n",
        "    grad_norms[rnn_type].append(np.linalg.norm(grad))\n",
        "\n",
        "plt.figure()\n",
        "for rnn_type in depths:\n",
        "  plt.plot(depths[rnn_type], grad_norms[rnn_type],\n",
        "           label=\"%s\" % rnn_type, alpha=0.7, lw=2)\n",
        "plt.legend()  \n",
        "plt.ylabel(\"$ \\\\| \\\\partial \\\\sum_i {c_{N}}_i / \\\\partial c_t \\\\|$\", fontsize=15)\n",
        "plt.xlabel(\"Steps through time - $t$\", fontsize=15)\n",
        "plt.xlim((1, SEQ_LENGTH-1))\n",
        "plt.title(\"Gradient magnitudes across time for: RNN-Type (forget_bias value)\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4v7TUtjKHD-",
        "colab_type": "text"
      },
      "source": [
        "### What do we learn from this?\n",
        "\n",
        "This particular experiment is an extremely simple surrogate for actual problem, but shows a few interesting aspects:\n",
        "\n",
        "* Is LSTM by construction free of *exploding* gradients too?\n",
        "* What are other ways of avoiding explosions you can think of?\n",
        "* Does initialisation (of gates here, but in general) matter a lot?\n",
        "* Does this look like a solution that can really scale time-wise? Say to be doing credit assignment through years of experience?\n",
        "* If not, what might be a next step?\n",
        "\n",
        "See http://proceedings.mlr.press/v37/jozefowicz15.pdf for a more detailed discussion of the effect of the forget gate bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf74B2tbcnYh",
        "colab_type": "text"
      },
      "source": [
        "# Ex. 3    Language Modelling\n",
        "\n",
        "Now we will train a character level RNN on text data - specifically Shakespeare sonnets. We will reuse the same concepts, such as teacher forcing and different types of RNN cores. \n",
        "\n",
        "At the end of the exercise, after you have filled in the TextModel class, you can train the model and see that in generates text that has sonnet structure and learns words.  You should focus on the TextModel class implementation, and leverage the code provided to do the training and visualization and data loading."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xmr1EjIQAM95",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Dataset wrapper\n",
        "\n",
        "class TinyShakespeare(dataset_shakespeare.TinyShakespeareDataset):\n",
        "  \n",
        "  def _find_starts(self):\n",
        "    starts = []\n",
        "    code = self._data_source._vocab_dict['|']\n",
        "    for i in range(len(self._flat_data)-4):\n",
        "      if code == self._flat_data[i] == self._flat_data[i+1]:\n",
        "        starts.append(i+2)\n",
        "    return np.array(starts)\n",
        "  \n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super(TinyShakespeare, self).__init__(*args, **kwargs)\n",
        "    self._starts = self._find_starts()\n",
        "    self._reset_head_indices()\n",
        "  \n",
        "  def cost(self, logits, target):\n",
        "    return super(TinyShakespeare, self).cost(logits, target) / self._num_steps\n",
        "  \n",
        "  def _reset_head_indices(self):\n",
        "    try:\n",
        "      self._head_indices = self._starts[np.random.randint(\n",
        "          low=0, high=len(self._starts), size=[self._batch_size])]\n",
        "    except:\n",
        "      self._head_indices = np.zeros(self._batch_size)\n",
        "      \n",
        "  def to_human_readable(self,\n",
        "                      data,\n",
        "                      label_batch_entries=True,\n",
        "                      indices=None,\n",
        "                      sep=\"\\n\",\n",
        "                      pretify=False):\n",
        "    new_data = super(TinyShakespeare, self).to_human_readable(data, \n",
        "                                                              label_batch_entries,\n",
        "                                                              indices, sep)\n",
        "    if pretify:\n",
        "      new_data = \"    \" + new_data.replace(\"|\", \"\\n    \")\n",
        "    return new_data\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hZVt_hIJsX1",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Playing with the dataset\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "# Create sonnet dataset object\n",
        "dataset_train = TinyShakespeare(\n",
        "  num_steps=64,\n",
        "  batch_size=1,\n",
        "  subset=\"train\",\n",
        "  random=False,\n",
        "  name=\"shake_train\")\n",
        "\n",
        "# Create TF ops to read sequences and their corresponding targets\n",
        "train_input_sequence, train_target_sequence = dataset_train()\n",
        "\n",
        "\n",
        "with tf.train.MonitoredSession() as sess:\n",
        "  \n",
        "  for k in range(1, 4):\n",
        "    sampled = sess.run(train_input_sequence)\n",
        "    if k == 1:\n",
        "      print(\"Network-friendly data\")\n",
        "      print(\"Data type\", type(sampled))\n",
        "      print(\"Data shape\", sampled.shape)\n",
        "      print(sampled)\n",
        "    print()\n",
        "    print(\"Iteration %d\" % k)\n",
        "    print(dataset_train.to_human_readable((sampled,),))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMUp4v2nE7vr",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title TextMode\n",
        "\n",
        "class TextModel(tf.layers.Layer):\n",
        "  \"\"\"A deep RNN model, for use on the Tiny Shakespeare dataset.\"\"\"\n",
        "\n",
        "  def __init__(self, num_hidden, depth, output_size, embedding_size=32,\n",
        "               rnn_core=rnn_cell.LSTMCell, name=\"text_model\"):\n",
        "    \"\"\"Constructs a `TextModel`.\n",
        "    Args:\n",
        "      num_hidden: Number of hidden units in each LSTM layer.\n",
        "      depth: Number of RNN layers.\n",
        "      output_size: Size of the output layer on top of the DeepRNN.\n",
        "      rnn_core: Reference to an RNNCore class such as LSTM.\n",
        "      name: Name of the module.\n",
        "    \"\"\"\n",
        "\n",
        "    super(TextModel, self).__init__(name=name)\n",
        "\n",
        "    self._num_hidden = num_hidden\n",
        "    self._depth = depth\n",
        "    self._output_size = output_size\n",
        "    self._rnn_core = rnn_core\n",
        "    \n",
        "    # This is the embedding function which is applied to the input\n",
        "    self._embed = tf.layers.Dense(embedding_size, activation=tf.nn.relu,\n",
        "                                  name=\"embed\")\n",
        "    \n",
        "    # This is the output-layer which has to be applied to the hidden state\n",
        "    # to get the outputs \n",
        "    self._output_module = tf.layers.Dense(self._output_size,\n",
        "                                          name=\"linear_output\")\n",
        "\n",
        "    self._subcores = [\n",
        "        self._rnn_core(self._num_hidden, name=\"rnn_{}\".format(i))\n",
        "        for i in range(self._depth)\n",
        "    ]    \n",
        "\n",
        "    # This is the RNN cell, which here is a Deep RNN.\n",
        "    self._core = rnn_cell.MultiRNNCell(self._subcores)\n",
        "\n",
        "\n",
        "  def call(self, one_hot_input_sequence):\n",
        "    \"\"\"Builds the deep RNN model sub-graph.\n",
        "    Args:\n",
        "      one_hot_input_sequence: A Tensor with the input sequence encoded as a\n",
        "        one-hot representation. Its dimensions should be `[truncation_length,\n",
        "        batch_size, output_size]`.\n",
        "    Returns:\n",
        "      Tuple of the Tensor of output logits for the batch, with dimensions\n",
        "      `[truncation_length, batch_size, output_size]`, and the\n",
        "      final state of the unrolled core,.\n",
        "    \"\"\"\n",
        "\n",
        "    input_shape = one_hot_input_sequence.get_shape()\n",
        "    batch_size = input_shape[1]\n",
        "\n",
        "    ######################\n",
        "    #   YOUR CODE HERE   #\n",
        "    ######################\n",
        "    # Use the RNN Model defined in self._core to implement the language model\n",
        "    # Hints:\n",
        "    # - compute input embeddings\n",
        "    # - get the intial states\n",
        "    # - unstack inputs\n",
        "    # - use tf.nn.static_rnn to connect the cell (self._core)\n",
        "    # - stack the outputs sequence\n",
        "    # - apply output layer (self._output_module) to cell states to get outputs \n",
        "\n",
        "    # - compute input embeddings (see self._embed)\n",
        "    # TODO: WRITE LINE OF CODE\n",
        "    input_sequence = None\n",
        "\n",
        "    # - get the intial states\n",
        "    # TODO: WRITE LINE OF CODE\n",
        "    initial_state = None\n",
        "    \n",
        "    # we need to do this because static_rnn expects a list of inputs,\n",
        "    # not a tensor\n",
        "    # e.g. rnn_input_sequence = tf.unstack(input_sequence)\n",
        "\n",
        "    # Use tf.contrib.rnn.static_rnn to apply the RNN to the sequence of inputs\n",
        "    # TODO: WRITE LINE OF CODE\n",
        "    output, final_state = None, None\n",
        "    \n",
        "    # we need to stack the outputs in order to pass all of them at once\n",
        "    # through the output layer because \n",
        "    # e.g. output_sequence = tf.stack(output)\n",
        "\n",
        "    # pass outputs states through the output-layer (self._output_module)\n",
        "    # TODO: WRITE LINE OF CODE\n",
        "    output_sequence_logits = None\n",
        "\n",
        "    # Checks\n",
        "    assert output_sequence_logits is not None, \"Connect the RNN\"\n",
        "\n",
        "    return output_sequence_logits, final_state\n",
        "\n",
        "\n",
        "  def generate_string(self, initial_logits, initial_state, sequence_length):\n",
        "    \"\"\"Builds sub-graph to generate a string, sampled from the model.\n",
        "    Args:\n",
        "      initial_logits: Starting logits to sample from.\n",
        "      initial_state: Starting state for the RNN core.\n",
        "      sequence_length: Number of characters to sample.\n",
        "    Returns:\n",
        "      A Tensor of characters, with dimensions `[sequence_length, batch_size,\n",
        "      output_size]`.\n",
        "      A Tensor of activities of hidden neurons\n",
        "    \"\"\"\n",
        "\n",
        "    current_logits = initial_logits\n",
        "    current_state = initial_state\n",
        "    activations = []\n",
        "    \n",
        "    generated_letters = []\n",
        "    for _ in range(sequence_length):\n",
        "      # Sample a character index from distribution.\n",
        "      char_index = tf.squeeze(tf.multinomial(current_logits, 1))\n",
        "      char_one_hot = tf.one_hot(char_index, self._output_size, 1.0, 0.0)\n",
        "      \n",
        "      generated_letters.append(char_one_hot)\n",
        "      \n",
        "      # Feed character back into the deep_lstm.\n",
        "      gen_out_seq, current_state = self._core(\n",
        "          self._embed(char_one_hot),\n",
        "          current_state)\n",
        "      current_logits = self._output_module(gen_out_seq)\n",
        "      \n",
        "      activations.append(current_state)\n",
        "\n",
        "    generated_string = tf.stack(generated_letters)\n",
        "    activations = tf.stack(activations)\n",
        "\n",
        "    return generated_string, activations\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7CVRaUW5nKU",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Building entire graph\n",
        "\n",
        "def build_graph(depth=3, batch_size=32, num_hidden=128,\n",
        "                truncation_length=64, sample_length=1000, max_grad_norm=5,\n",
        "                initial_learning_rate=0.1, reduce_learning_rate_multiplier=0.1,\n",
        "                optimizer_epsilon=0.01, rnn_core=rnn_cell.LSTMCell):\n",
        "\n",
        "  # Get datasets.\n",
        "  dataset_train = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"train\",\n",
        "      random=False,\n",
        "      name=\"shake_train\")\n",
        "\n",
        "  dataset_valid = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"valid\",\n",
        "      random=False,\n",
        "      name=\"shake_valid\")\n",
        "\n",
        "  dataset_test = TinyShakespeare(\n",
        "      num_steps=truncation_length,\n",
        "      batch_size=batch_size,\n",
        "      subset=\"test\",\n",
        "      random=False,\n",
        "      name=\"shake_test\")\n",
        "\n",
        "  # Define model.\n",
        "  with tf.variable_scope(\"main\", reuse=tf.AUTO_REUSE):\n",
        "    model = TextModel(\n",
        "        num_hidden=num_hidden,\n",
        "        depth=depth,\n",
        "        output_size=dataset_valid.vocab_size,\n",
        "        rnn_core=rnn_core)\n",
        "\n",
        "    # Get the training loss.\n",
        "    train_input_sequence, train_target_sequence = dataset_train()\n",
        "    train_output_sequence_logits, train_final_state = model(train_input_sequence)  \n",
        "    train_loss = dataset_train.cost(train_output_sequence_logits,\n",
        "                                    train_target_sequence)\n",
        "\n",
        "    # Get the validation loss.\n",
        "    valid_input_sequence, valid_target_sequence = dataset_valid()\n",
        "    valid_output_sequence_logits, _ = model(valid_input_sequence)\n",
        "    valid_loss = dataset_valid.cost(valid_output_sequence_logits,\n",
        "                                    valid_target_sequence)\n",
        "\n",
        "    # Get the test loss.\n",
        "    test_input_sequence, test_target_sequence = dataset_test()\n",
        "    test_output_sequence_logits, _ = model(test_input_sequence) \n",
        "    test_loss = dataset_test.cost(test_output_sequence_logits,\n",
        "                                  test_target_sequence)\n",
        "\n",
        "    # Build graph to sample some strings during training.\n",
        "    initial_logits = train_output_sequence_logits[truncation_length - 1]\n",
        "    train_generated_string, activations = model.generate_string(\n",
        "        initial_logits=initial_logits,\n",
        "        initial_state=train_final_state,\n",
        "        sequence_length=sample_length)\n",
        "  \n",
        "  # Set up global norm clipping of gradients.\n",
        "  trainable_variables = tf.trainable_variables()\n",
        "  \n",
        "  raw_grads = tf.gradients(train_loss, trainable_variables)\n",
        "  if max_grad_norm is not None:\n",
        "    grads, _ = tf.clip_by_global_norm(raw_grads, max_grad_norm)\n",
        "  else:\n",
        "    grads = raw_grads\n",
        "\n",
        "  # Get learning rate and define annealing.\n",
        "  learning_rate = tf.get_variable(\n",
        "      \"learning_rate\",\n",
        "      shape=[],\n",
        "      dtype=tf.float32,\n",
        "      initializer=tf.constant_initializer(initial_learning_rate),\n",
        "      trainable=False)\n",
        "  \n",
        "  reduce_learning_rate = learning_rate.assign(\n",
        "      learning_rate * reduce_learning_rate_multiplier)\n",
        "\n",
        "  # Get training step counter.\n",
        "  global_step = tf.get_variable(\n",
        "      name=\"global_step\",\n",
        "      shape=[],\n",
        "      dtype=tf.int64,\n",
        "      initializer=tf.zeros_initializer(),\n",
        "      trainable=False,\n",
        "      collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
        "\n",
        "  # Define optimizer and training step.\n",
        "  optimizer = tf.train.AdamOptimizer(\n",
        "      learning_rate)\n",
        "  \n",
        "  train_step = optimizer.apply_gradients(\n",
        "      zip(grads, trainable_variables),\n",
        "      global_step=global_step)\n",
        "\n",
        "  graph_tensors = {\n",
        "      \"train_loss\": train_loss,\n",
        "      \"valid_loss\": valid_loss,\n",
        "      \"test_loss\": test_loss,\n",
        "      \"train_generated_string\": train_generated_string,\n",
        "      \"reduce_learning_rate\": reduce_learning_rate,\n",
        "      \"global_step\": global_step,\n",
        "      \"train_step\": train_step,\n",
        "      \"raw_gradients\": raw_grads,\n",
        "      \"activations\": activations,\n",
        "  }\n",
        "  \n",
        "  # Return dataset_train for translation to human readable text.\n",
        "  return graph_tensors, dataset_train\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwTVV-km6cOz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run experiment\n",
        "NUM_TRAINING_ITERATIONS = 2000 #@param\n",
        "REPORT_INTERVAL = 500 #@param\n",
        "REDUCE_LEARNING_RATE_INTERVAL = 1500 #@param\n",
        "\n",
        "DEPTH = 2 #@param\n",
        "BATCH_SIZE = 64 #@param\n",
        "NUM_HIDDEN = 128 #@param\n",
        "TRUNCATION_LENGTH = 64 #@param Sequence size for training.\n",
        "SAMPLE_LENGTH = 500 #@param Sequence size for sampling.\n",
        "MAX_GRAD_NORM = None #@param Gradient clipping norm limit.\n",
        "LEARNING_RATE = 0.01 #@param Optimizer learning rate.\n",
        "REDUCE_LEARNING_RATE_MULTIPLIER = 0.1 #@param Learning rate is multiplied by this when reduced.\n",
        "OPTIMIZER_EPSILON = 1e-8 #@param Adam epsilon\n",
        "\n",
        "RNN_CORE = \"rnn_cell.LSTMCell\" #@param ['RNN', 'rnn_cell.LSTMCell', 'rnn_cell.GRUCell']\n",
        "RNN_CORE = eval(RNN_CORE)\n",
        "\n",
        "TESTING_ITERATIONS = 1000 #@param\n",
        "\n",
        "\n",
        "print(\"Building the graph\")\n",
        "print()\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "graph_tensors, dataset_train = build_graph(\n",
        "    depth=DEPTH, \n",
        "    batch_size=BATCH_SIZE,\n",
        "    num_hidden=NUM_HIDDEN,\n",
        "    truncation_length=TRUNCATION_LENGTH,\n",
        "    sample_length=SAMPLE_LENGTH,\n",
        "    max_grad_norm=MAX_GRAD_NORM,\n",
        "    initial_learning_rate=LEARNING_RATE,\n",
        "    reduce_learning_rate_multiplier=REDUCE_LEARNING_RATE_MULTIPLIER,\n",
        "    optimizer_epsilon=OPTIMIZER_EPSILON,\n",
        "    rnn_core=RNN_CORE)\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# Train the network.\n",
        "print(\"Training starts...\")\n",
        "print()\n",
        "\n",
        "sess = tf.train.MonitoredSession()\n",
        "sess.run(init)\n",
        "\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for train_iteration in range(NUM_TRAINING_ITERATIONS):\n",
        "  if train_iteration % REPORT_INTERVAL == 1:\n",
        "    train_loss_v, valid_loss_v, _, raw_gradients = sess.run(\n",
        "        (graph_tensors[\"train_loss\"],\n",
        "         graph_tensors[\"valid_loss\"],\n",
        "         graph_tensors[\"train_step\"],\n",
        "         graph_tensors[\"raw_gradients\"]))\n",
        "\n",
        "    train_generated_string_v = sess.run(\n",
        "        graph_tensors[\"train_generated_string\"])\n",
        "\n",
        "    train_generated_string_human = dataset_train.to_human_readable(\n",
        "        (train_generated_string_v, 0), False, indices=[0], pretify=True)\n",
        "\n",
        "    print(\"%d: Training loss %f. Validation loss %f.\" % (\n",
        "                    train_iteration,\n",
        "                    train_loss_v,\n",
        "                    valid_loss_v))\n",
        "\n",
        "    print(\"Gradient norm: %f\" % np.linalg.norm(np.concatenate(\n",
        "        [g.reshape(-1) for g in raw_gradients])))\n",
        "    print()\n",
        "    print(train_generated_string_human)\n",
        "    print()\n",
        "    \n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(train_losses, c='b', label='Train')\n",
        "    plt.plot(valid_losses, c='g', label='Valid')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  else:\n",
        "    train_loss_v, valid_loss_v, _ = sess.run((graph_tensors[\"train_loss\"],\n",
        "                                              graph_tensors[\"valid_loss\"],\n",
        "                                              graph_tensors[\"train_step\"]))\n",
        "    \n",
        "  train_losses.append(train_loss_v)\n",
        "  valid_losses.append(valid_loss_v)\n",
        "\n",
        "\n",
        "  if (train_iteration + 1) % REDUCE_LEARNING_RATE_INTERVAL == 0:\n",
        "    sess.run(graph_tensors[\"reduce_learning_rate\"])\n",
        "    print(\"Reducing learning rate.\")\n",
        "\n",
        "test_losses = []    \n",
        "for k in range(TESTING_ITERATIONS):    \n",
        "  test_loss_v = sess.run(graph_tensors[\"test_loss\"])\n",
        "  test_losses.append(test_loss_v)\n",
        "  \n",
        "print(\"Test loss %f\" % np.mean(test_losses))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_uGGyiZyI3w",
        "colab_type": "text"
      },
      "source": [
        "## Ex 3.1   Analysis of single neurons and gates\n",
        "\n",
        "We will now look at the individual activations of neurons in a Recurrent network. For this to work, you need to have completed the previous exercise in which you expose the network activations, as well as train a model.\n",
        "\n",
        "For a similar analysis, see [this paper](https://arxiv.org/pdf/1506.02078.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gU06mESEcoT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title String plot function\n",
        "\n",
        "def string_plot(chars, values, title=None):\n",
        "  \"\"\"\n",
        "  Given a string \"chars\" and a vector of numbers \"values\" of the same length\n",
        "  displays the string, using \"|\" as EOL symbol, and colors each character\n",
        "  background using corresponding value in values\n",
        "  \"\"\"\n",
        "  \n",
        "  assert len(chars) == len(values)\n",
        "  \n",
        "  lines = []\n",
        "  line = \"\"\n",
        "  for char in chars:\n",
        "    if char != '|':\n",
        "      line += char\n",
        "    else:\n",
        "      line += \" \"\n",
        "      lines.append(line)\n",
        "      line = \"\"\n",
        "  lines.append(line)\n",
        "  \n",
        "  height = len(lines) \n",
        "  width = max(map(len, lines))\n",
        "    \n",
        "  data = np.zeros((height, width))\n",
        "  data[:,:] = np.nan\n",
        "  \n",
        "  pos = 0\n",
        "  for lid, line in enumerate(lines):\n",
        "    data[lid, :len(line)] = values[pos: pos+len(line)]\n",
        "    pos = pos+len(line)\n",
        "    \n",
        "  assert pos == len(values)\n",
        "    \n",
        "  plt.figure(figsize=(width * 0.3, height * 0.3))\n",
        "  plt.title(title)\n",
        "  plt.imshow(data.reshape((height, width)), interpolation='none',\n",
        "             cmap='Reds', alpha=0.5)\n",
        "  plt.axis('off')\n",
        "  \n",
        "  for lid, line in enumerate(lines):\n",
        "    for cid, char in enumerate(line):\n",
        "      plt.text(cid-0.2,lid+0.2,char,color='k',fontsize=9)\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sy-fC04-gXN",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Generate some new text and corresponding activations\n",
        "\n",
        "text, activations = sess.run([\n",
        "    graph_tensors['train_generated_string'],\n",
        "    graph_tensors['activations']\n",
        "])\n",
        "\n",
        "\n",
        "print(dataset_train.to_human_readable((text, 0), False, indices=[0],\n",
        "                                      pretify=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIoafi_KDJ3g",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Display activities of random neurons\n",
        "\n",
        "NUMBER_OF_NEURONS = 10  #@param {type:\"integer\"}\n",
        "STATE_DIM = 0  #@param {type:\"integer\"}\n",
        "\n",
        "if len(activations.shape) == 5:\n",
        "  selected_activations = activations[:, :, STATE_DIM, 0, :]\n",
        "else:\n",
        "  selected_activations = activations[:, :, 0, :]\n",
        "\n",
        "chars = list(dataset_train.to_human_readable(\n",
        "        (text, 0), False, indices=[0]))\n",
        "\n",
        "neurons_ids = np.random.choice(range(activations.shape[-1]), \n",
        "                               min(NUMBER_OF_NEURONS, activations.shape[-1]),\n",
        "                               replace=False)\n",
        "\n",
        "for layer_id in reversed(range(selected_activations.shape[1])): \n",
        "  values = selected_activations[:, layer_id, :].T\n",
        "  for neuron in sorted(neurons_ids):\n",
        "    string_plot(chars, \n",
        "                values[neuron],\n",
        "                title='Layer %d, Neuron %d, State %d' \n",
        "                % (layer_id, neuron, STATE_DIM))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTOoND9nNGyp",
        "colab_type": "text"
      },
      "source": [
        "### What kind of neurons can we expect to find?\n",
        "\n",
        "* Lots of counting neurons (their activity is just growing/decreasing independently from input)\n",
        "* Names neuron - activates around names of people in the play, such as HAMLET: or JOHN OF GAUNT:\n",
        "* Line width neuron - with activity proportional to the length of the current line (number of charaters since last \"|\")\n",
        "* Paragraph length neuron - activity proportional to the length of the paragraph in lines\n",
        "* Special character neurons - such as coding for probability of generating \":\"\n",
        "* Many, many mixtures of the above\n",
        "\n",
        "Note, that if neurons like these do not appear it does not mean that network does not \"know\" these elements. Highly discriminative, single neuron decoupling is not something neural networks are trained to do, it is just an empirical observation, shared across many domains (cat neurons in visual classifiers etc.). Knowledge can be represented in many other ways, in particular the fact that it is represented in a single neuron does not mean network does not have a distributed \"backup\" of the same knowledge somewhere else.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtOHP2haXQ9F",
        "colab_type": "text"
      },
      "source": [
        "## Ex 3.2   Analysis of the state dynamics\n",
        "\n",
        "In this exercise, we will visualize the activations in a different way, by projecting them to 2 dimensions, via dimensionality reduction. \n",
        "\n",
        "When using different projection techniques, you willl see different results. For example, PCA will display the directions with most variance in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uksDGFBp69Kz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Compute 2D projection of the hidden state\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE, Isomap\n",
        "\n",
        "projector = 'TSNE' #@param ['PCA', 'TSNE', 'Isomap']\n",
        "projector_fun = eval(projector)\n",
        "\n",
        "kwargs = {\n",
        "    'TSNE': {'perplexity': 50},\n",
        "    'PCA': {},\n",
        "    'Isomap': {}\n",
        "}\n",
        "\n",
        "values = selected_activations.reshape(selected_activations.shape[0], -1)\n",
        "projector = projector_fun(n_components=2, **kwargs[projector])\n",
        "\n",
        "values_2d = projector.fit_transform(values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_iOk3TDDyBM",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Data plotter\n",
        "\n",
        "splits = {\n",
        "    'nothing': ('$', ),\n",
        "    'lines': ('|', ),\n",
        "    'paragraphs': ('|', '|', ),\n",
        "}\n",
        "\n",
        "split_criterion = 'paragraphs' #@param ['nothing', 'lines', 'paragraphs']\n",
        "\n",
        "parts = []\n",
        "part = \"\"\n",
        "for cid, char in enumerate(chars):\n",
        "  match = True\n",
        "  part += char\n",
        "  for k in range(min(len(chars)-cid, len(splits[split_criterion]))):\n",
        "    if chars[cid+k] != splits[split_criterion][k]:\n",
        "      match = False\n",
        "  if match:\n",
        "    parts.append(len(part))\n",
        "    part = \"\"\n",
        "    \n",
        "if len(part)>0:    \n",
        "  parts.append(len(part))    \n",
        "  \n",
        "\n",
        "plt.figure()\n",
        "plt.title('Hidden activities, colored according to %s' \n",
        "          % split_criterion.replace('_', ' '))\n",
        "current = 0\n",
        "for part_id, part in enumerate(parts):\n",
        "  plt.plot(values_2d[current:current+part,0],\n",
        "           values_2d[current:current+part,1],\n",
        "           label='Part %d' % part_id)\n",
        "  current += part\n",
        "  \n",
        "plt.legend(bbox_to_anchor=(1.2, 1.05))  \n",
        "plt.show()\n",
        "\n",
        "current = 0\n",
        "for part_id, part in enumerate(parts):\n",
        "  print('Part %d' % part_id)\n",
        "  print(dataset_train.to_human_readable((text[current:current+part], 0),\n",
        "                                              False, indices=[0],\n",
        "                                              pretify=True))\n",
        "  print()\n",
        "  \n",
        "  current += part\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZmBGOnXxRsI",
        "colab_type": "text"
      },
      "source": [
        "### So what am I looking at?\n",
        "\n",
        "2D projections of high-dimensional spaces are always loosing a lot of information, however the general structure can still be recovered. Here, one can see that both paragraph-splits and line-splits can be decoded by just looking at the dynamics of the hidden state, giving more insights into internals of an RNN. Note, that contrary to single-neuron analysis, here we are truly looking at the whole picture, thus what is observed is likely behind the dynamics of this model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpXpLbLVobgN",
        "colab_type": "text"
      },
      "source": [
        "# Done."
      ]
    }
  ]
}