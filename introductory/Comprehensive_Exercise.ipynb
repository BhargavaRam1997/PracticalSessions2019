{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Comprehensive_Exercise.ipynb","version":"0.3.2","provenance":[{"file_id":"1r4IWzK2gHwXnbumu-1avZ9flg8ixREaM","timestamp":1530768533770}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python2","display_name":"Python 2"}},"cells":[{"cell_type":"markdown","metadata":{"id":"iYx6oEVKcE_L","colab_type":"text"},"source":["# Exercise: putting everything together\n","\n","In this you will write code for a model that learns to classify mnist digits. You will use tensorflow, tracking training progress with matplotlib.\n","\n","For each sub-exercise, you have seen an example solution for it in one of the colabs leading up to this one."]},{"cell_type":"code","metadata":{"id":"TGBJLkR_cI3L","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import random\n","import seaborn as sns\n","import numpy as np\n","import tensorflow as tf\n","import datetime\n","\n","from matplotlib import pyplot as plt\n","from google.colab import files\n","from scipy.stats import multivariate_normal\n","from IPython.display import clear_output, Image, display, HTML\n","\n","sns.set_style('ticks')\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5gkBQpjJlCgP","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nO_tMPdncmVy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":153},"outputId":"4c684feb-ae3f-4f97-af15-4ee7c80cf85a","executionInfo":{"status":"ok","timestamp":1561538210102,"user_tz":-60,"elapsed":3049,"user":{"displayName":"David Szepesvari","photoUrl":"https://lh4.googleusercontent.com/-djDv8j7EUOg/AAAAAAAAAAI/AAAAAAAAAEs/s9Ds-tUarwE/s64/photo.jpg","userId":"06728915078212315022"}}},"source":["# Fetch the mnist data from tf.keras.datasets.mnist.\n","\n","mnist_train, mnist_test = tf.keras.datasets.mnist.load_data()\n","\n","# Check what the data is like:\n","print('Training dataset:')\n","train_input, train_label = mnist_train\n","print('* input shape:', train_input.shape)\n","print('* input min, mean, max:', train_input.min(), train_input.mean(), train_input.max())\n","print('* input dtype:', train_input.dtype)\n","print('* label shape:', train_label.shape)\n","print('* label min, mean, max:', train_label.min(), train_label.mean(), train_label.max())\n","print('* label dtype:', train_label.dtype)\n","\n","test_input, test_label = mnist_test\n","print('Number of test examples:', test_input.shape[0])"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Training dataset:\n","* input shape: (60000, 28, 28)\n","* input min, mean, max: 0 33.318421449829934 255\n","* input dtype: uint8\n","* label shape: (60000,)\n","* label min, mean, max: 0 4.4539333333333335 9\n","* label dtype: uint8\n","Number of test examples: 10000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"utL4ZmLrepoH","colab_type":"text"},"source":["Normalize the data into the \\[0, 1\\] interval. It's also a good idea to check the class distribution, but here we know that this is OK.\n","\n"]},{"cell_type":"code","metadata":{"id":"60_4wXEPe7Ig","colab_type":"code","colab":{}},"source":["# Normalize both train_input and test_input so that it is in [0, 1].\n","#\n","# Also ensure the following data types:\n","#\n","# * train_input and test_input need to be np.float32.\n","# * the labels need to be converted to np.int32.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDwRkDiYfzVO","colab_type":"code","colab":{}},"source":["# We can visualize the first few training examples using matplotlib.imshow()\n","# in combination with the gallery function we defined.\n","#\n","# Copy the gallery function in this cell.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1WQD1huVgV8Y","colab_type":"code","colab":{}},"source":["# Show the first 6 training images on a 1x6 grid.\n","# Remember to use grayscale plotting.\n","# Also print their corresponding labels in the same order.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6VZdwYo_fUpo","colab_type":"code","colab":{}},"source":["# Write a function that turns the data into tensorflow datasets and into\n","# tensors corresponding to batches of examples, returning these tensors.\n","#\n","# The train data should be\n","#\n","# * shuffled across the full dataset\n","# * repeated indefinitely\n","# * batched at size 64.\n","#\n","# Simply batch the test data.\n","#\n","# IMPORTANT: Add a final (singleton) axis to the inputs; the conv nets that\n","# we will use will expect this.\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d3JcANwNfHuQ","colab_type":"code","colab":{}},"source":["# Creata function that returns a network with the following structure:\n","#\n","# 1. Conv2D with 16 filters, kernel shape 3, stride 1, padding 'SAME'\n","# 2. max pooling with window_shape [3, 3], srides [2, 2], padding 'SAME'\n","# 3. ReLU\n","# 4. Conv2D with 16 filters, kernel shape 3, stride 1, padding 'SAME'\n","# 5. Flatten the final conv features using snt.BatchFlatten\n","# 5. A Dense layer with output_size = 10, the number of classes.\n","#\n","# Make sure you use variable scoping to be able to share the underlying\n","# variables.\n","\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRp2hrGofH7f","colab_type":"code","colab":{}},"source":["tf.reset_default_graph()\n","(train_inputs, train_labels), (test_inputs, test_labels) = get_tf_data()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7daVkyoqS9p","colab_type":"code","colab":{}},"source":["# * Get the output of the network on the training data,\n","# * and the output of the *same* network with same weights on the test data.\n","# * Use the `tf.nn.sparse_softmax_cross_entropy_with_logits` op to define the loss\n","# * Define the train_op that minimizes the loss (averaged over the batch)\n","#   using the `GradientDescentOptimizer`. Set the learning rate to 0.01.\n","# * Get the initialization op.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wvmlucn6vbSD","colab_type":"code","colab":{}},"source":["# Write a function that takes a list of losses and plots them.\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tufk2Xa2qTEI","colab_type":"code","colab":{}},"source":["# Run the training loop, keeping track of losses and potentially the accuracy\n","# on the training set. Plot the loss curve intermittently.\n","#\n","# The simplest solution would add a new plot with each plotting call. You\n","# can play with the frequency of plotting (and recording) a bit in order\n","# to find something that works.\n","#\n","# Based on the loss curves, decide how to set your total number of training\n","# iterations. Once you are satified, add some code that evaluates your\n","# prediction accuracy (not loss!) on the test set.\n","#\n","# Note that the outputs from the network are logits; for prediction accuracy\n","# we can pick the most likely label and see if it is correct.\n","\n","# The accuracy (on the training set) you should expect:\n","#\n","# * Roughly 90% after 1000 training steps.\n","# * 96-97% after 8k training steps.\n","#\n","# First iterate with 1k steps, if that works, train for 8k. 8k steps will\n","# be roughly 8 minutes on CPU.\n","#\n","# The final test accuracy should also be ~96%.\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0fwSrI-c2Cn3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xvt4bOeP1Bbo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChrJA2KOqTMD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}