{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Exercises] EEML - BLR 2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP69ddGg42Df",
        "colab_type": "text"
      },
      "source": [
        "# Bayesian Logistic Regression\n",
        "\n",
        "Contact: mihaelacr@google.com\n",
        "\n",
        "Aim of this colab:\n",
        "Logistic regression with a twist! We will use variational inference to learn a distribution over the learned parameters.\n",
        "\n",
        "Througout this work we will be focusing on a binary classification task, on \n",
        "a small dataset (the UCD breast cancer dataset). \n",
        "\n",
        "\n",
        "## Logistic regression\n",
        "\n",
        "Learn the weights which best classify the $(x, y)$ pairs in the dataset. \n",
        "The weights ($w$) and biases $b$, form the parameters which are learned via the following maximization problem:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{p^*(x, y)} \\log p(y|x, \\theta) = \\\\\n",
        " \\mathbb{E}_{p^*(x, y)} \\left[y \\log \\sigma(w x + b) + (1 - y) \\log(1- \\sigma(w x + b)\\right] \n",
        "\\end{equation}\n",
        "\n",
        "Here $\\sigma$ denotes the sigmoid function.\n",
        "\n",
        "## Bayesian logistic regression - computing the evidence lower bound\n",
        "\n",
        "Learn a distribution over the weights which best classify the $(x, y)$ pairs in the dataset.\n",
        "\n",
        "We now want to learn the distributional parameters of $q(w)$ in order to maximize:\n",
        "\n",
        "\\begin{equation}\n",
        "\\mathbb{E}_{p^*(x, y)} \\log p_w(y| x) = \\\\\n",
        "\\mathbb{E}_{p^*(x, y)} \\log \\int p(y|x, w) p(w) \\delta w = \\\\\n",
        "\\mathbb{E}_{p^*(x, y)} \\log \\int p(y|x, w) p(w) \\frac{q(w)}{q(w)} \\delta w \\ge \\\\\n",
        "\\mathbb{E}_{p^*(x, y)} \\mathbb{E}_{q(w)} \\log \\left[p(y| x, w) \\frac{p(w)}{q(w)} \\right]= \\\\\n",
        "\\mathbb{E}_{q(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y|x, w)  -  KL(q(w)||p(w)) = \\\\\n",
        " \\mathbb{E}_{q(w)}\\mathbb{E}_{p^*(x, y)} \\left[y \\log \\sigma(w x + b) + (1 - y) \\log(1- \\sigma(w x + b)\\right] -  KL(q(w)||p(w))\n",
        "\\end{equation}\n",
        "\n",
        "In Bayesian Logistic Regression, we thus learn a distribution over parameters\n",
        "$w$ which can explain the data, while staying close to a chosen prior $p(w)$.\n",
        "\n",
        "Throughout this lab, we will use Gaussian Distributions for $p(w)$ and $q(w)$.\n",
        "\n",
        "For more details, see \n",
        "[this paper](https://pdfs.semanticscholar.org/e407/ea7fda6d152d2186f4b5e27aa04ec2d32dcd.pdf) \n",
        "or this tutorial\n",
        "[https://www.ece.rice.edu/~vc3/elec633/logistic.pdf].\n",
        "\n",
        "### Gradient estimation\n",
        "\n",
        "In Bayesian Logistic Regression, we aim to learn the parameters of the distribution $q(w)$. We will call these parameters $\\theta$. In the case of a \n",
        "Gaussian distribution, these will be the mean and covariance matrix of a\n",
        "multivariate distribution. Since we will use stochastic gradient descent to learn $\\theta$, we need to be able to compute the gradients with respect to \n",
        "$\\theta$ of our objective. Since our objective contains an expectation with respect to $q(w)$, this can be challenging. To illustrate this, from now on we will denote $q(w)$ as $q_{\\theta}(w)$.\n",
        "\n",
        "Specifically, we are interested in:\n",
        "\\begin{equation}\n",
        "  \\nabla_{\\theta} \\left[\\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y|x, w)  -  KL(q_{\\theta}(w)||p(w))\\right]\n",
        "\\end{equation}\n",
        "\n",
        "Since in we will be using Gaussian distributions for $q_{\\theta}(w)$ and $p(w)$ \n",
        "$KL(q_{\\theta}(w)||p(w))$ can be computed in closed form and we can rely on \n",
        "TensorFlow's automatic differentiation to compute \n",
        "$\\nabla_{\\theta} KL(q_{\\theta}(w)||p(w))$.\n",
        "\n",
        "Let's now turn attention to computing $\\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w)$. Unlike the KL, the integral \n",
        "$\\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y|x, w)$\n",
        "is not tractable, and thus cannot be computed in closed form. Thus, we are interested in other approaches to compute the gradient.\n",
        "\n",
        "### Reinforce gradient estimation\n",
        "\n",
        "A first approach to compute an estimate of the gradient is to use the\n",
        "REINFORCE gradient estimator:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w)  = \\\\\n",
        "  \\nabla_{\\theta} \\int q_{\\theta}(w) \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w) \\delta w = \\\\\n",
        "  \\int \\nabla_{\\theta} \\left[q_{\\theta}(w) \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w) \\right] \\delta w = \\\\\n",
        " \\int \\nabla_{\\theta} \\left[q_{\\theta}(w)\\right] \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w) \\delta w = \\\\\n",
        " \\int q_{\\theta}(w) \\nabla_{\\theta} \\left[\\log q_{\\theta}(w)\\right] \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w)  \\delta w = \\\\\n",
        " \\mathbb{E}_{q_{\\theta}(w)} \\left[ \\nabla_{\\theta} \\log q_{\\theta}(w) \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w) \\right]\n",
        "\\end{equation}\n",
        "\n",
        "We can use samples from $q_{\\theta}(w)$ to compute the Monte Carlo estimate of\n",
        "the last integral, to get an unbiased estimator of the true gradient. The more \n",
        "samples we use to estimate the integral, the more accurate the gradient estimate\n",
        "will be.\n",
        "\n",
        "\n",
        "### Pathwise gradient estimation (reparametrization)\n",
        "\n",
        "In the REINFORCE estimator, we did not use any knowledge of the variational\n",
        "distribution $q_{\\theta}(w)$.\n",
        "For a Gaussian distribution, we can use the reparametrization trick to obtain\n",
        "an unbiased gradient estimator of  $\\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w)$.\n",
        "\n",
        "To do so, we will use the fact that \n",
        "\\begin{equation}\n",
        "    z \\sim N(\\mu, \\sigma), z = \\mu + \\epsilon \\sigma, {\\text {with }} \\epsilon \\sim N(0, 1)\n",
        "\\end{equation}\n",
        "\n",
        "Namely:\n",
        "\\begin{equation}\n",
        "  \\nabla_{\\theta} \\mathbb{E}_{q_{\\theta}(w)} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w)  = \\\\\n",
        "  \\nabla_{\\theta} \\mathbb{E}_{p(\\epsilon)} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, \\mu + \\epsilon \\sigma)  = \\\\\n",
        "  \\mathbb{E}_{p(\\epsilon)} \\nabla_{\\theta} \\mathbb{E}_{p^*(x, y)} \\log p(y| x, \\mu + \\epsilon \\sigma)\n",
        "\\end{equation}\n",
        "\n",
        "Note that we were able to move the gradient inside the integral since \n",
        "$p(\\epsilon)$ does not depend on $\\theta$.\n",
        "To estimate the last integral, we can use a Monte Carlo estimate using samples\n",
        "of $p(\\epsilon)$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PxAlISU7AYV",
        "colab_type": "text"
      },
      "source": [
        "## Your tasks\n",
        "\n",
        "\n",
        "*   Define the Gaussian posterior distribution.\n",
        "*   Fill in the code for the ELBO\n",
        "*   Fill in the reinforce function to create the surrogate loss\n",
        "*   Fill in the loss function for reparametrization\n",
        "*   Visualize the effects of batch size, learning rates and number of posterior samples on the learned model, as well as gradient variance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH8xRXJVy-be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import enum\n",
        "import numpy as np\n",
        "\n",
        "import math\n",
        "from sklearn import datasets\n",
        "from sklearn import linear_model\n",
        "import collections\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import seaborn as sns\n",
        "\n",
        "tfd = tfp.distributions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JD97evxTyirI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={\"lines.linewidth\": 2.8}, font_scale=2)\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhV6gBY_zRzC",
        "colab_type": "text"
      },
      "source": [
        "## Get the dataset and visualize it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzrvgE3e4lX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(normalize = True):\n",
        "    data = datasets.load_breast_cancer()\n",
        "\n",
        "    print(data.target_names)\n",
        "    print(data.feature_names)\n",
        "\n",
        "    features = np.array(data.data, dtype=np.float32)\n",
        "    targets = np.array(data.target, dtype=np.float32)\n",
        "\n",
        "    if normalize:\n",
        "        # Note: the data dimensions have very different scales.\n",
        "        # We normalize by the mean and scale of the entire dataset.\n",
        "        features = features - features.mean(axis=0)\n",
        "        features = features / features.std(axis=0)\n",
        "\n",
        "    # Add a dimension of 1 (bias).\n",
        "    features = np.concatenate((features, np.ones((features.shape[0], 1))), axis=1)\n",
        "    features = np.array(features, dtype=np.float32)\n",
        "  \n",
        "    return features, targets, data.feature_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44Jrompq1iTY",
        "colab_type": "text"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "This datasets has meaningful features: we are trying to classify based on these features whether someone has breast cancer or not. We can look at the\n",
        "features we are using to classify the data below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgQ8-mnH4pTT",
        "colab_type": "code",
        "outputId": "903a61cc-5fcd-41f6-de0f-0aa7c00b2c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "features, targets, feature_names = get_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['malignant' 'benign']\n",
            "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
            " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
            " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
            " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
            " 'smoothness error' 'compactness error' 'concavity error'\n",
            " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
            " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
            " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
            " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXDbcOfl1P1J",
        "colab_type": "text"
      },
      "source": [
        "### Trivial baseline\n",
        "\n",
        "How well can we do with a constant classifier?\n",
        "\n",
        "If we look at the average label, we can see how many 1s are in the data, so we know that our classifier has to get more than that accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9iwLLowE4z8",
        "colab_type": "code",
        "outputId": "74754b07-71f0-4ce4-eda2-300bc5719410",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.mean(targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6274165"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aA8nn2zy64F3",
        "colab_type": "code",
        "outputId": "c199ba51-2ce0-4d75-994c-ea1ed5aa1dd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "features.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 31)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwJmx4bR5ooL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_size = features.shape[0]\n",
        "data_dims = features.shape[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzVQ6Z7szaqY",
        "colab_type": "text"
      },
      "source": [
        "## Logistic regression baseline - how well can we do on this dataset?\n",
        "\n",
        "Let's look at a simple, non Bayesian classification approach to get an idea of how well we can do on the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eft7ZXm64k_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = linear_model.LogisticRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lt9jDrhC4vYY",
        "colab_type": "code",
        "outputId": "0c0fe6b9-1e86-4416-f1c0-a3775e4b9e69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "lr.fit(features, targets)\n",
        "lr.score(features, targets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9876977152899824"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jv5OaALJ0mnB",
        "colab_type": "text"
      },
      "source": [
        "## Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "th2ZTE8S0ooa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_shape_list(tensor):\n",
        "  return tensor.shape.as_list()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D0d_XbW1a9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sklearn_data_as_tensors(\n",
        "      features, targets, batch_size, dataset_name='breast_cancer'):\n",
        "  \"\"\"Read sklearn datasets as tf.Tensors.\n",
        "\n",
        "  Args:\n",
        "    batch_size: Integer or None. If None, the entire dataset is used.\n",
        "    dataset_name: A string, the name of the dataset.\n",
        "\n",
        "  Returns:\n",
        "    A tuple of size two containing two tensors of rank 2 `[B, F]`, the data\n",
        "    features and targets.\n",
        "  \"\"\"\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
        "  if batch_size:\n",
        "    # Shuffle, repeat, and batch the examples.\n",
        "    batched_dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
        "  else:\n",
        "    batch_size = features.shape[0]\n",
        "    batched_dataset = dataset.repeat().batch(batch_size)\n",
        "\n",
        "  iterator = batched_dataset.make_one_shot_iterator()\n",
        "  batch_features, batch_targets = iterator.get_next()\n",
        "\n",
        "  data_dim = features.shape[1]\n",
        "  batch_features.set_shape([batch_size, data_dim])\n",
        "  batch_targets.set_shape([batch_size])\n",
        "\n",
        "  return batch_features, batch_targets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd923omSze8D",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Logistic Regression - define the prior and the posterior\n",
        "\n",
        "We will learn a distribution over parameters, so we will first define prior distribution, and then a posterior distribution over parameters which we \n",
        "will learn.\n",
        "\n",
        "Both of these distibutions will be Gaussian."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnekwuYW7zoq",
        "colab_type": "text"
      },
      "source": [
        "## Task: implement the posterior distribution\n",
        "\n",
        "You have to implement two functions \n",
        "\n",
        "\n",
        "1.   multi_normal, which takes as arguments the mean and the log scale of the normal distribution, and returns a tensorflow distribution object. Have a look at the TensorFlow distibutions package. \n",
        "2.   diagonal_gaussian_posterior which returns a tuple of two elements, one being the posterior distribution (a call to multi_normal) and the learned variables. Your task here is to define the variables we will learn. The argument `data_dims` tells you the size of the learned parameters, and thus the size of the variables of the distribution).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iugzul802iEC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_normal(loc, log_scale):\n",
        "  raise NotImplementedError(\n",
        "        'Implement the definition of multi normal')\n",
        "\n",
        "def diagonal_gaussian_posterior(data_dims):\n",
        "  mean = None\n",
        "  log_scale = None\n",
        "  learned_vars = [mean, log_scale]\n",
        "  return multi_normal(loc=mean, log_scale=log_scale), learned_vars"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJ80lrXNvDFi",
        "colab_type": "text"
      },
      "source": [
        "## Bayesian Logistic Regression - the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v2PO8UC3qxi",
        "colab_type": "text"
      },
      "source": [
        "### Hypers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg6V1IGO3r8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 500\n",
        "NUM_POSTERIOR_SAMPLES = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8AwA1Stfvbe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prior = multi_normal(loc=tf.zeros(data_dims), log_scale=tf.zeros(data_dims))\n",
        "posterior, learned_vars  = diagonal_gaussian_posterior(data_dims)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DimyoNcuwmS9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _predictions(logits):\n",
        "  return tf.to_float(logits >= 0)\n",
        "\n",
        "def _accuracy(targets, predictions):\n",
        "  # `predictions` have rank 2: batch_size, num_posterior samples.\n",
        "  # We expand dims the targets to compute the accuracy per sample.\n",
        "  targets = tf.expand_dims(targets, axis=1)\n",
        "  return tf.reduce_mean(tf.to_float(tf.equal(targets, predictions)))\n",
        "\n",
        "def linear_model(data, targets, posterior_samples):\n",
        "  num_posterior_samples = tf.shape(posterior_samples)[0]\n",
        "  logits = tf.matmul(data, posterior_samples, transpose_b=True)\n",
        "  # Make targets [B, 1] to use broadcasting.\n",
        "  targets = tf.expand_dims(targets, axis=1)\n",
        "  targets = targets * tf.ones([1, num_posterior_samples])\n",
        "\n",
        "  log_probs = - tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "      labels=targets, logits=logits)\n",
        "  return log_probs, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W42eKG3v9MQn",
        "colab_type": "text"
      },
      "source": [
        "## Task: Run the model and return the components of the elbo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_eDfavd0XJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(posterior, num_samples):\n",
        "  # Sample from the posterior to obtain a set of parameters used for\n",
        "  # prediction. Use `num_samples` for the number of samples. \n",
        "  posterior_samples = None\n",
        "\n",
        "  # Compute the log probs of the data under all the models we have sampled.\n",
        "  # These tensors are [num_samples, B]. Use the features and targets tensors.\n",
        "  log_probs, logits = None\n",
        "\n",
        "  # Compute the KL between the posterior and the prior.\n",
        "  # Note: since we use Gaussian distributions, the closed form of the \n",
        "  # KL can be computed. Remember that distributions are objects in TensorFlow!\n",
        "  kl = None\n",
        "\n",
        "  # Sum over data. Normalize by dataset size to ensure that the gradients\n",
        "  # are not very different in magnitude when we change batch size.  \n",
        "  param_log_probs = tf.reduce_mean(log_probs, axis=0) * dataset_size\n",
        "  param_log_probs.shape.assert_is_compatible_with([num_samples])\n",
        "\n",
        "  return param_log_probs, kl, posterior_samples, logits "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vN5pWJrnwf4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_log_probs, kl, posterior_samples, logits = run_model(\n",
        "      posterior, NUM_POSTERIOR_SAMPLES)\n",
        "predictions = _predictions(logits)\n",
        "accuracy = _accuracy(targets=targets, predictions=predictions)\n",
        "accuracy = tf.reduce_mean(accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE855A8__FbD",
        "colab_type": "text"
      },
      "source": [
        "## Task: compute the elbo from the log probs and the KL \n",
        "\n",
        "`per_sample_elbo` should have shape [num_samples], since we obtain a model prediction for each of the sampled parameters.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J_L8B81_JBn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per_sample_elbo = None\n",
        "elbo = tf.reduce_mean(per_sample_elbo)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfwdvB2l154M",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2bpGEYV5cj-",
        "colab_type": "text"
      },
      "source": [
        "## Training the model via reparametrization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ-7BFe7M-iB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Stochastic loss depends on the output of a sampling operation  (posterior samples)\n",
        "# but tensorflow implements reparametrization by default.\n",
        "per_sample_reparametrization_loss = - per_sample_elbo\n",
        "reparametrization_loss = tf.reduce_mean(per_sample_reparametrization_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBLPB0rNNOZS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "reparam_min_op = optimizer.minimize(reparametrization_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIlHWgFjNfez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArWZCPDRNXhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "reparam_accuracies = []\n",
        "reparam_kls = []\n",
        "reparam_elbos = []\n",
        "\n",
        "for i in xrange(NUM_ITERATIONS):\n",
        "  sess.run(reparam_min_op)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    reparam_acc, reparam_kl, reparam_elbo = sess.run([accuracy, kl, elbo])\n",
        "    reparam_accuracies += [reparam_acc]\n",
        "    reparam_kls += [reparam_kl]\n",
        "    reparam_elbos += [reparam_elbo]\n",
        "\n",
        "    print('Iteration {}. Elbo {}. KL {}'.format(\n",
        "        i, reparam_elbo, reparam_kl))\n",
        "\n",
        "reparam_learned_mean, reparam_learned_log_scale = sess.run(learned_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tbpa02jv_Dv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(reparam_elbos, label='ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].set_ylim((-500, -50))\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(reparam_kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(reparam_accuracies, label='Accuracy')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo-9zFbt5kii",
        "colab_type": "text"
      },
      "source": [
        "## Training the model via reinforce"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k25ZYFOU___p",
        "colab_type": "text"
      },
      "source": [
        "## Task: define the surrogate reinforce loss for the log prob term.\n",
        "\n",
        "Note: we do not need to use reinforce the KL term, since we can compute the KL analytically for the two Gaussians, and can use standard backprop. \n",
        "\n",
        "For the reinforce implementation, instead of changing the gradients, we will change the loss such that TensorFlow's automatic differentiation does the right thing. This is called a surrogate loss. \n",
        "\n",
        "The gradient we want to obtain is the following (since we want to minimize the loss, we add a minus):\n",
        "\\begin{equation}\n",
        "- \\mathbb{E}_{q_{\\theta}(w)} \\left[ \\nabla_{\\theta} \\log q_{\\theta}(w) \\mathbb{E}_{p^*(x, y)} \\log p(y| x, w) \\right]\n",
        "\\end{equation}\n",
        "\n",
        "so we need to construct a loss which has this gradient, by ensuring that the gradients will flow through the $\\log q_{\\theta}(w)$ term, but not the rest. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVSL5MhtSZj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since TensorFlow does automatic differentation, we have to use \n",
        "# tf.stop_gradient to ensure the gradient only flows to\n",
        "# the log_{q_\\theta}(w) term.\n",
        "per_sample_reinforce_loss =  None   # Shape [num_samples, batch size].\n",
        "# Reduce now over the number of samples.\n",
        "reinforce_loss = tf.reduce_mean(per_sample_reinforce_loss) + kl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WO4jbPvdTMd5",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
        "reinforce_min_op = optimizer.minimize(reinforce_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GWpybmi6TMeD",
        "colab": {}
      },
      "source": [
        "NUM_ITERATIONS = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "smLiPL7kTMeO",
        "colab": {}
      },
      "source": [
        "reinforce_accuracies = []\n",
        "reinforce_kls = []\n",
        "reinforce_elbos = []\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "for i in xrange(NUM_ITERATIONS):\n",
        "  sess.run(reinforce_min_op)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    reinforce_acc, reinforce_kl, reinforce_elbo = sess.run([accuracy, kl, elbo])\n",
        "    reinforce_accuracies += [reinforce_acc]\n",
        "    reinforce_kls += [reinforce_kl]\n",
        "    reinforce_elbos += [reinforce_elbo]\n",
        "\n",
        "    print('Iteration {}. Elbo {}. KL {}'.format(\n",
        "        i, reinforce_elbo, reinforce_kl))\n",
        "\n",
        "reinforce_learned_mean, reinforce_learned_log_scale = sess.run(learned_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGE32mI8s1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(reinforce_elbos, label='ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].set_ylim((-500, -50))\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(reinforce_kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(reinforce_accuracies, label='Accuracy')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQJ0iFv-6Xa-",
        "colab_type": "text"
      },
      "source": [
        "## Task: Analysis\n",
        "\n",
        "* How sensitive are the different estimators to the number of posterior samples used? Try 1, 10, 100.\n",
        "* How sensitive are the different estimators to the learning rate used? Try 1e-4, 1e-3 and 1e-2 for both estimators.\n",
        "* How sensitive are the different estimators to batch size? Try batch size 10 for both estimators."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCcTNQsO5tE1",
        "colab_type": "text"
      },
      "source": [
        "# Visualize learned parameters\n",
        "\n",
        "* Let's visualize the learned parameters, as well as check the different uncertanties around different parameters\n",
        "* Can we now see which parameters are most important in the classification problem?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDHFNDkj7SPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learned_mean = reparam_learned_mean\n",
        "learned_scale = np.exp(reparam_learned_log_scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5WoqVaZ9vGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PARAM_INDEX = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wlSrBlz9Eeg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gaussian(x, mu, sig):\n",
        "    return np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxXoXzGo9FHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = learned_mean[PARAM_INDEX]\n",
        "scale = learned_scale[PARAM_INDEX]\n",
        "x_values = np.linspace(-3, 3, num=1000)\n",
        "y_values = [gaussian(x, mean, scale) for x in x_values]\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(x_values, y_values)\n",
        "plt.vlines(\n",
        "      learned_mean[PARAM_INDEX], \n",
        "      ymin=0, \n",
        "      ymax=gaussian(mean, mean, scale),\n",
        "      colors='g',\n",
        "      alpha=0.8)\n",
        "\n",
        "plt.grid('off')\n",
        "x_std_values = np.linspace(mean - scale, mean + scale, num=1000)\n",
        "y_std_values = [gaussian(x, mean, scale) for x in x_std_values]\n",
        "plt.fill_between(x_std_values, y_std_values, 0, alpha=0.3, color='b')\n",
        "\n",
        "plt.title('Parameter name= {}'.format(feature_names[PARAM_INDEX]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gksr5bG06AQT",
        "colab_type": "text"
      },
      "source": [
        "## Visualize gradient variance - which estimator has lower variance?\n",
        "\n",
        "* Let's assess the gradient variance per of each estimator, by looking at how much gradients vary for each estimators. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUc9PsoXAAui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def per_batch_grads(ys, xs):\n",
        "  \"\"\"Computes the gradients of ys[i] wrt xs.\"\"\"\n",
        "  grads = []\n",
        "\n",
        "  for y in tf.unstack(ys, axis=0):\n",
        "    grads.append(tf.squeeze(tf.gradients(y, xs)[0]))\n",
        "  return tf.stack(grads, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ93_xwrA-OZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_grad_var(grads, param_index):\n",
        "  return np.mean(grads[:, param_index]), np.std(grads[:, param_index])**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ee7sAMBAP7V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per_batch_reinforce_grads = per_batch_grads(per_sample_reinforce_loss, learned_vars[0])\n",
        "per_batch_reinforce_grads.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fqjku6G5EmUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per_batch_reparam_grads = per_batch_grads(per_sample_reparametrization_loss, learned_vars[0])\n",
        "per_batch_reparam_grads.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC5LrYVYAj66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reinforce_grads = []\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "for i in xrange(NUM_ITERATIONS):\n",
        "  sess.run(reinforce_min_op)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    reinforce_grads.append(sess.run(per_batch_reinforce_grads))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fifRpABRF5j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reparam_grads = []\n",
        "\n",
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())\n",
        "\n",
        "for i in xrange(NUM_ITERATIONS):\n",
        "  sess.run(reparam_min_op)\n",
        "\n",
        "  if i % 10 == 0:\n",
        "    reparam_grads.append(sess.run(per_batch_reparam_grads))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77PN9dRaCxF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PARAM_INDEX = 19"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXj7oslkBu9j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reinforce_grad_vars = []\n",
        "for timestep_grad in reinforce_grads:\n",
        "  _, reinforce_grad_var = compute_grad_var(timestep_grad, PARAM_INDEX)\n",
        "  reinforce_grad_vars.append(reinforce_grad_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emEg7lOMGnSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reparam_grad_vars = []\n",
        "for timestep_grad in reparam_grads:\n",
        "  _, reparam_grad_var = compute_grad_var(timestep_grad, PARAM_INDEX)\n",
        "  reparam_grad_vars.append(reparam_grad_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPsGH7elC0gH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(7,5))\n",
        "plt.plot(reinforce_grad_vars, label='reinforce')\n",
        "plt.plot(reparam_grad_vars, label='reparametrization')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('It')\n",
        "plt.ylabel('Gradient variance')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYfwIlI1Guoa",
        "colab_type": "text"
      },
      "source": [
        "### Task: Analysis\n",
        "\n",
        "* What do you notice about gradient variance as training progresses?\n",
        "* What do you notice about the variance of the different estimators?\n",
        "* How do batch size and number of posterior samples affect the variance of the different estimators?\n",
        "* How does gradient variance affect convergence?\n",
        "* Which gradient estimator would you use in practice for this problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1A6FIHHnXEnW",
        "colab_type": "text"
      },
      "source": [
        "## More\n",
        "\n",
        "You can find similar experiments with more analysis and more gradient estimators in [this paper](https://arxiv.org/pdf/1906.10652.pdf). See Section 8.3."
      ]
    }
  ]
}