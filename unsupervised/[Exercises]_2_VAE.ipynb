{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Exercises] EEML VAE tutorial - 2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8PEhbMy8Q-H",
        "colab_type": "text"
      },
      "source": [
        "Contact: mihaelacr@google.com\n",
        "\n",
        "## Contents\n",
        "\n",
        "* amortized variational inference (VAEs)\n",
        "* improving amortized variational inference using KL annleaing\n",
        "* improving amortized variational inference using constraint optimization\n",
        "\n",
        "## Tasks\n",
        "  * [Code] fill in the Elbo for the VAE\n",
        "  * [Analysis] discuss the difference between sample and reconstruction quality for VAEs, gradient estimation\n",
        "  * [Code] define the update operation for the KL coefficient\n",
        "  * [Analysis] discuss the effect of the KL annealing\n",
        "  * [Code] implement the Lagrangian optimization (GECO) \n",
        "  * [Analysis] what is the effect of the *learned* Lagrangian on the KL schedule? How would you set the value for the constraint? What are the advantages of using constrained optimization? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDHkWjCVFOxI",
        "colab_type": "text"
      },
      "source": [
        "## Imports and set up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hk4fInRg8NWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import google3\n",
        "\n",
        "import tensorflow as tf\n",
        "import sonnet as snt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Plotting library.\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Tensorflow probability utilities\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "tfd = tfp.distributions\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVkOCcFZ9HlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.set(rc={\"lines.linewidth\": 2.8}, font_scale=2)\n",
        "sns.set_style(\"whitegrid\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOEsOiP9L4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Don't forget to select GPU runtime environment in Runtime -> Change runtime type\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3loVhgk-H2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gallery(array, ncols=10, rescale=False):\n",
        "    \"\"\"Data visualization code.\"\"\"\n",
        "    if rescale:\n",
        "      array = (array + 1.) / 2\n",
        "    nindex, height, width, intensity = array.shape\n",
        "    nrows = nindex//ncols\n",
        "    assert nindex == nrows*ncols\n",
        "    # want result.shape = (height*nrows, width*ncols, intensity)\n",
        "    result = (array.reshape(nrows, ncols, height, width, intensity)\n",
        "              .swapaxes(1,2)\n",
        "              .reshape(height*nrows, width*ncols, intensity))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyRPYX3WWnPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_digits(axis, digits, title=''):\n",
        "  axis.axis('off')\n",
        "  ncols = int(np.sqrt(digits.shape[0]))\n",
        "  axis.imshow(gallery(digits, ncols=ncols).squeeze(axis=2), \n",
        "                 cmap='gray')\n",
        "  axis.set_title(title, fontsize=15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsKLwGBa5iWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_latent_interpolations(generator, prior, session):\n",
        "  a = np.linspace(0.0, 1.0, BATCH_SIZE)\n",
        "  a = np.expand_dims(a, axis=1)\n",
        "\n",
        "  first_latents = prior.sample()[0]\n",
        "  second_latents = prior.sample()[0]\n",
        "\n",
        "  # To ensure that the interpolation is still likely under the Gaussian prior,\n",
        "  # we use Gaussian interpolation - rather than linear interpolation.\n",
        "  interpolations =  np.sqrt(a) * first_latents + np.sqrt(1 - a) * second_latents\n",
        "\n",
        "  ncols = int(np.sqrt(BATCH_SIZE))\n",
        "  samples_from_interpolations = generator(interpolations)\n",
        "  samples_from_interpolations_np = sess.run(samples_from_interpolations)\n",
        "  plt.gray()\n",
        "  axis = plt.gca()\n",
        "  show_digits(\n",
        "     axis, samples_from_interpolations_np, title='Latent space interpolations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQOPrU_tfljy",
        "colab_type": "text"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iFjsBLsDCCAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "NUM_LATENTS = 10\n",
        "TRAINING_STEPS = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S62pxDHf2C_t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DECODER_VARIABLE_SCOPE = \"decoder\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W528LifA2-mW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cfQlEC07pal",
        "colab_type": "text"
      },
      "source": [
        "## Get the data\n",
        "\n",
        "We will use the MNIST dataset. Luckly, TensorFlow comes with a simple way to load it. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EHWPAsd7u50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWipXtlq8Ya9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mnist.train.images.shape)\n",
        "print(type(mnist.train.images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWded8OecPFg",
        "colab_type": "text"
      },
      "source": [
        "### Transform the data from numpy arrays to in graph tensors.\n",
        "\n",
        "This allows us to use TensorFlow datasets, which ensure that a new batch from the data is being fed at each session.run. This means that we do not need to use feed_dicts to feed data to each session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmrdDH2ibvGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_tf_data_batch(np_data, shuffle=True):  \n",
        "  # Reshape the data to image size.\n",
        "  images = np_data.reshape((-1, 28, 28, 1))\n",
        "  \n",
        "  # Create the TF dataset.   \n",
        "  dataset = tf.data.Dataset.from_tensor_slices(images)\n",
        "  \n",
        "  # Shuffle and repeat the dataset for training.\n",
        "  # This is required because we want to do multiple passes through the entire\n",
        "  # dataset when training.\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(100000).repeat()\n",
        "    \n",
        "  # Batch the data and return the data batch.\n",
        "  one_shot_iterator = dataset.batch(BATCH_SIZE).make_one_shot_iterator()\n",
        "  data_batch = one_shot_iterator.get_next()\n",
        "  return data_batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKwY_jfDHZdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "print(real_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E54OTbDJ91h2",
        "colab_type": "text"
      },
      "source": [
        "# Amortized variational inference (VAEs)\n",
        "\n",
        "\n",
        "Instead of learning one set of posterior variables per data point, we can use function approximation to learn the distributional variables. Specifically, the posterior parameters for $x_i$ will be the output of a *learned* function $f_\\theta(x_i)$, where $\\theta$ are parameters shared across all data points. Can you think of why this is useful?\n",
        "\n",
        "\n",
        "<h2 align=\"center\"></h2> <img src=\"http://elarosca.net/vae.png?format=100w\" width=500 >\n",
        "\n",
        "\n",
        "Objective - maximize: \n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} \\log p_{\\theta}(x|z)  - \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z))\n",
        "\\end{equation}\n",
        "\n",
        "For more information, see: \n",
        "  * https://arxiv.org/abs/1312.6114"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UqJLmAhoCy0M",
        "colab_type": "text"
      },
      "source": [
        "## Define the decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXI5bRIXqmdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def standard_decoder(z):\n",
        "  with tf.variable_scope(DECODER_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    h = tf.layers.dense(z, 7 * 7 * 64, activation=tf.nn.relu)\n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, 7, 7, 64])\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=1,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=None,  # Do not activate the last layer.\n",
        "          padding='same')(h)\n",
        "    return tf.distributions.Bernoulli(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lum1PLiW9I79",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def big_decoder(z):\n",
        "  with tf.variable_scope(DECODER_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    h = tf.layers.dense(z, 4000, activation=tf.nn.relu)\n",
        "    h = tf.layers.dense(z, 4000, activation=tf.nn.relu)\n",
        "    h = tf.layers.dense(z, 4000, activation=tf.nn.relu)\n",
        "    h = tf.layers.dense(h, 7 * 7 * 64, activation=tf.nn.relu)\n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, 7, 7, 64])\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2DTranspose(\n",
        "          filters=1,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=None,  # Do not activate the last layer.\n",
        "          padding='same')(h)\n",
        "    return tf.distributions.Bernoulli(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dsn12nG3NQKg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ENCODER_VARIABLE_SCOPE = 'encoder'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys5kCiS3NKQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encoder(x):\n",
        "  with tf.variable_scope(ENCODER_VARIABLE_SCOPE, reuse=tf.AUTO_REUSE):\n",
        "    \n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=8,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(x)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=16,\n",
        "          kernel_size=5,\n",
        "          strides=2,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    h = tf.layers.Conv2D(\n",
        "          filters=32,\n",
        "          kernel_size=5,\n",
        "          strides=1,\n",
        "          activation=tf.nn.relu,\n",
        "          padding='same')(h)\n",
        "    \n",
        "    out_shape = 1\n",
        "    for s in h.shape.as_list()[1:]:\n",
        "      out_shape*= s\n",
        "    \n",
        "    h = tf.reshape(h, shape=[BATCH_SIZE, out_shape])\n",
        "    mean = tf.layers.dense(h, NUM_LATENTS, activation=None)\n",
        "    scale = tf.layers.dense(h, NUM_LATENTS, activation=None)\n",
        "    return multi_normal(loc=mean, log_scale=scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dc-I57ZvtX0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def multi_normal(loc, log_scale):\n",
        "  # We model the latent variables as independent\n",
        "  return tfd.Independent(\n",
        "      distribution=tfd.Normal(loc=loc, scale=tf.exp(log_scale)),\n",
        "      reinterpreted_batch_ndims=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idi-DgDzl_iE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_prior():\n",
        "  # Zero mean, unit variance prior.\n",
        "  prior_mean = tf.zeros(shape=(BATCH_SIZE, NUM_LATENTS), dtype=tf.float32)\n",
        "  prior_log_scale = tf.zeros(shape=(BATCH_SIZE, NUM_LATENTS), dtype=tf.float32)\n",
        "\n",
        "  return multi_normal(prior_mean, prior_log_scale)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLhqAAqucoLt",
        "colab_type": "text"
      },
      "source": [
        "## Task: define the elbo (variational bound) terms\n",
        "\n",
        "Compared to the BLR colab, the posterior is over latent variables, and they are conditioned on the input. \n",
        "\n",
        "You now have to define the two terms of the elbo: the log prob term, and the KL term. Remember that for the KL term you can compute the analytical KL since the prior and the posterior are Gaussian."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JheUZIkxvlDl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bound_terms(data_batch, variational_posterior, decoder_fn):\n",
        "  latent_sample = None\n",
        "  # Compute the log probs of the data (log p(x|z)). \n",
        "  # Hint: Use the decoder and look at the shape assertion below!\n",
        "  all_log_probs = None\n",
        "\n",
        "  # Since we used broadcasting, log_prob of a Bernoulli will return a log_prob for \n",
        "  # each dimension.\n",
        "  all_log_probs.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "  # Reduce sum over the data dimensions.\n",
        "  # This is needed because we used independent distributions for each of the \n",
        "  # data pixels.\n",
        "  likelihood_term = None\n",
        "\n",
        "  # Reduce mean over the batch dimensions\n",
        "  likelihood_term = tf.reduce_mean(likelihood_term)\n",
        "\n",
        "  # Compute the KL divergence. \n",
        "  # Hint: the posterior is a distribution object (check its member functions)!\n",
        "  kl_term = None\n",
        "  kl_term.shape.assert_is_compatible_with([BATCH_SIZE])\n",
        "\n",
        "  # Reduce over the batch dimension.\n",
        "  kl_term = tf.reduce_mean(kl_term)\n",
        "  \n",
        "  return likelihood_term, kl_term"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbRi8ejYmezC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "print(real_data.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mv1BbElmQnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prior = make_prior()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy-QiXwSOGXY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The posterior is conditioned on the data: q(z|x).\n",
        "variational_posterior = encoder(real_data)\n",
        "decoder = standard_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaW-j91gOuU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Maximize the data likelihodd and minimize the KL divergence between the prior \n",
        "# and posterior. \n",
        "likelihood_term, kl_term = bound_terms(\n",
        "    real_data, variational_posterior, decoder)\n",
        "train_elbo = likelihood_term - kl_term\n",
        "loss =  - train_elbo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5KtRZWKaO6IY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2BquI1DPF3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now perform joint optimization on the encoder and decoder variables.\n",
        "optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "decoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DECODER_VARIABLE_SCOPE)\n",
        "encoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=ENCODER_VARIABLE_SCOPE)\n",
        "autoencoder_vars = encoder_vars + decoder_vars\n",
        "update_op = optimizer.minimize(loss, var_list=autoencoder_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xlVu9cRPg6f",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdB-pHnQPcuU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrGGwzfDPdtb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(update_op)\n",
        "  \n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl  = sess.run(\n",
        "          [loss, likelihood_term, kl_term])\n",
        "    print('Iteration {}. Loss {}. KL {}'.format(\n",
        "          i, iteration_loss, iteration_kl))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WO8-O_MVp9h3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Visualize the loss in time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlA-gGsnqA-Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(losses, label='Negative ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zs7PaG_j9Gac",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples and latent interpolations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz0anr_cPskE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJIu_K8KP5tE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yumeCpiF9Dj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUuLShU8MWpn",
        "colab_type": "text"
      },
      "source": [
        "## Questions about amortized variational inference\n",
        "\n",
        "* What do you observe about sample quality and reconstruction quality?\n",
        "* What do you observe about the ELBO and KL term?\n",
        "* Which gradient estimation method is used to compute the gradients with respect to the encoder parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UdvLfi1oysb",
        "colab_type": "text"
      },
      "source": [
        "# KL annealing\n",
        "\n",
        "Objective - maximize: \n",
        "\\begin{equation}\n",
        " \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} \\log p_\\theta(x|z)  - \\alpha \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z))\n",
        "\\end{equation}\n",
        "\n",
        "Where $\\alpha$ changes during training, to weigh in the KL term more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqOVwsdFo2PK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zPGuSGA2JHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "prior = make_prior()\n",
        "decoder = standard_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLSjAR9g2ZqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kl_coefficient = tf.Variable(\n",
        "    tf.zeros(shape=(1), dtype=tf.float32), \n",
        "    trainable=False,\n",
        "    name='kl_coeff')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug28JT08fDua",
        "colab_type": "text"
      },
      "source": [
        "## Task: define the update schedule for the coefficient of the KL\n",
        "\n",
        "We want the kl coefficient (`kl_coefficient`) to increase linearly by `kl_step` at each iteration. To achieve this, we need to define an update operation in TensorFlow, which we will run manually in the training loop (see below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCnknHU0e71N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kl_step = 1.0 / TRAINING_STEPS\n",
        "update_kl_coeff = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQbSvlj92JeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "variational_posterior = encoder(real_data)\n",
        "# Maximize the data likelihodd and minimize the KL divergence between the prior \n",
        "# and posterior. \n",
        "likelihood_term, kl_term = bound_terms(\n",
        "    real_data, variational_posterior, decoder)\n",
        "train_elbo = likelihood_term - kl_coefficient * kl_term\n",
        "loss =  - train_elbo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "L4ZYTrg-2UtS",
        "colab": {}
      },
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4NBpfmRB2Ute",
        "colab": {}
      },
      "source": [
        "# We now perform joint optimization on the encoder and decoder variables.\n",
        "optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "decoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DECODER_VARIABLE_SCOPE)\n",
        "encoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=ENCODER_VARIABLE_SCOPE)\n",
        "autoencoder_vars = encoder_vars + decoder_vars\n",
        "variables_update_op = optimizer.minimize(loss, var_list=autoencoder_vars)\n",
        "\n",
        "# Ensure that a variable update is followed by an update in the KL coefficient.\n",
        "with tf.control_dependencies([variables_update_op]):\n",
        "  update_op = tf.identity(update_kl_coeff)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0V1KL4wK2Utm"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6-zDc0Lv2Uto",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Voo_XAxa2Uts",
        "colab": {}
      },
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(update_op)\n",
        "  \n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl  = sess.run(\n",
        "          [loss, likelihood_term, kl_term])\n",
        "    print('Iteration {}. Loss {}. KL {}'.format(\n",
        "          i, iteration_loss, iteration_kl))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e0ILfEYa5Z0R",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*8,5))\n",
        "\n",
        "axes[0].plot(losses, label='Negative ELBO')\n",
        "axes[0].set_title('Time', fontsize=15)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(kls, label='KL')\n",
        "axes[1].set_title('Time', fontsize=15)\n",
        "axes[1].legend()\n",
        "\n",
        "axes[2].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[2].set_title('Time', fontsize=15)\n",
        "axes[2].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2Knn2ra65Z0a",
        "colab": {}
      },
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c-v7DO-b5Z0h",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nw8H8x2c8zaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZbZIhli8GBy",
        "colab_type": "text"
      },
      "source": [
        "## Questions about KL annealing\n",
        "\n",
        "* What do you observe about the KL behaviour throughout training as opposed to amortized variational inference without any KL annealing? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXTPxRYeo2k0",
        "colab_type": "text"
      },
      "source": [
        "# Constrained optimization\n",
        "\n",
        "Instead of using KL annealing, constrained optimization can be used to automatically tuned the relative weight of the likelihood and kl terms. This removes the need to manually create an optimization schedule, which can be problem specific.\n",
        "\n",
        "The objective now becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{minimize } \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z)) \\text{ such that }  \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x) \\log p_\\theta(x|z)} > \\alpha \n",
        "\\end{equation}\n",
        "\n",
        "This can be solved using the use of Lagrange multipliers. The objective becomes:\n",
        "\n",
        "\\begin{equation}\n",
        "  \\text{minimize }  \\mathbb{E}_{p^*(x)} KL(q(z|x)||p(z)) + \\lambda  (\\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z)))\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The difference compared to the KL annealing is that:\n",
        "\n",
        "   * $\\lambda$ is a learned parameter - it will be learned using stochastic gradient descent, like the network parameters.  The difference is that the lagrangian has to solve a maximization problem. You can see this intuitively: the graadient with respect to $\\lambda$ in the objective above is $\\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z))$. If $ \\mathbb{E}_{p^*(x)} \\mathbb{E}_{q(z|x)} (\\alpha - \\log p_\\theta(x|z))> 0$, the constraint is not being satisfied, so the value of the lagrangian needs to increase. This will be done by doing gradient ascent, instead of gradient descent. Note that for $\\lambda$ to be a valid lagranian in a minimization problem, it has to be positive.\n",
        "   * The practicioner has to specify the hyperparameter $\\alpha$, which determines the reoncstruction quality of the model.\n",
        "   * the coefficient is in front of the likelihood term, not the KL term. This is mainly for convenience, as it is easier to specify the hyperparameter $\\alpha$ for the likelihood (reconstruction loss).\n",
        "\n",
        "\n",
        "For more assumptions made by this method, see the Karush–Kuhn–Tucker conditions.\n",
        "\n",
        "For more information, see: \n",
        "  * http://bayesiandeeplearning.org/2018/papers/33.pdf\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "55yrz5b7Kktr",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YzYEtVIyKkty",
        "colab": {}
      },
      "source": [
        "real_data = make_tf_data_batch(mnist.train.images)\n",
        "prior = make_prior()\n",
        "decoder = standard_decoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsT4rTSkg_jb",
        "colab_type": "text"
      },
      "source": [
        "## Task: define the lagrangian multiplier\n",
        "\n",
        "The lagarngian multiplier is a learned variable, and always has to be positive. We ideally would like to have the coefficient initialized around 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-kJd3IKCKkt5",
        "colab": {}
      },
      "source": [
        "# Unlike in the KL annealing case, we learn the coefficient.\n",
        "lagrangian_var = None\n",
        "# Ensure that the lagrangian is positive and has stable dynamics.\n",
        "lagrangian = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7y7SkHsALzZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# How good do we want the reconstruction loss to be?\n",
        "# We can look at previous runs to get an idea what a reasonable value would be.\n",
        "reconstruction_target = - 90"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XG45n9zkKkt9",
        "colab": {}
      },
      "source": [
        "variational_posterior = encoder(real_data)\n",
        "likelihood_term, kl_term = bound_terms(\n",
        "    real_data, variational_posterior, decoder)\n",
        "# Note: now the elbo is a different quanitity to what we optimize.\n",
        "train_elbo = likelihood_term - kl_term\n",
        "loss =  kl_term + lagrangian * (reconstruction_target - likelihood_term )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uUoBwfHdKkuB",
        "colab": {}
      },
      "source": [
        "samples = decoder(prior.sample()).mean()\n",
        "samples.shape.assert_is_compatible_with([BATCH_SIZE, 28, 28, 1])\n",
        "\n",
        "reconstructions = decoder(variational_posterior.sample()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1e2b1BrOcW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.trainable_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ItrKEIEQKkuG",
        "colab": {}
      },
      "source": [
        "# We now perform joint optimization on the encoder and decoder variables.\n",
        "optimizer = tf.train.AdamOptimizer(0.001, beta1=0.9, beta2=0.9)\n",
        "decoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=DECODER_VARIABLE_SCOPE)\n",
        "encoder_vars = tf.get_collection(\n",
        "    tf.GraphKeys.TRAINABLE_VARIABLES, scope=ENCODER_VARIABLE_SCOPE)\n",
        "autoencoder_vars = encoder_vars + decoder_vars\n",
        "autoencoder_variables_update_op = optimizer.minimize(\n",
        "      loss, var_list=autoencoder_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxgrnnSphWRa",
        "colab_type": "text"
      },
      "source": [
        "## Task: optimize the lagrangian multiplier\n",
        "\n",
        "The lagrange multiplier needs to maximize the loss (not minimise it). Because of this, we use a separate optimizer which minimises the negative loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJkKBHDdig3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lagrangian_optimizer = tf.train.GradientDescentOptimizer(0.001)\n",
        "\n",
        "# Ensure that a variable update is followed by an update to the Lagrangian.\n",
        "with tf.control_dependencies([autoencoder_variables_update_op]):\n",
        "  update_op = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BhWHvrlBKkuJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Yp1z9724KkuK",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()\n",
        "\n",
        "# Initialize all variables\n",
        "sess.run(tf.initialize_all_variables())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "phV3D_PNKkuO",
        "colab": {}
      },
      "source": [
        "# %hide_pyerr  # - uncomment to interrupt training without a stacktrace\n",
        "losses = []\n",
        "kls = []\n",
        "likelihood_terms = []\n",
        "lagrangian_values = []\n",
        "\n",
        "for i in xrange(TRAINING_STEPS):\n",
        "  sess.run(update_op)\n",
        "  \n",
        "  \n",
        "  if i % 100 == 0:\n",
        "    iteration_loss, iteration_likelihood, iteration_kl, lag_val  = sess.run(\n",
        "          [loss, likelihood_term, kl_term, lagrangian])\n",
        "    print('Iteration {}. Loss {}. KL {}. Lagrangian {}'.format(\n",
        "          i, iteration_loss, iteration_kl, lag_val))\n",
        "    losses.append(iteration_loss)\n",
        "    kls.append(iteration_kl)\n",
        "    likelihood_terms.append(iteration_likelihood)\n",
        "    lagrangian_values.append(lag_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Aj6HI5uqKkuW",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(2, 2, figsize=(2*8, 2* 5))\n",
        "\n",
        "axes[0, 0].plot(losses, label='Negative ELBO')\n",
        "axes[0, 0].set_title('Time', fontsize=15)\n",
        "axes[0, 0].legend()\n",
        "\n",
        "axes[0, 1].plot(kls, label='KL')\n",
        "axes[0, 1].set_title('Time', fontsize=15)\n",
        "axes[0, 1].legend()\n",
        "\n",
        "axes[1, 0].plot(likelihood_terms, label='Likelihood Term')\n",
        "axes[1, 0].set_title('Time', fontsize=15)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "axes[1, 1].plot(lagrangian_values, label='Lagrangian Values')\n",
        "axes[1, 1].set_title('Time', fontsize=15)\n",
        "axes[1, 1].legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys5QOAYH8uvK",
        "colab_type": "text"
      },
      "source": [
        "### Generate samples and latent interpolations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EP9dSqfKkua",
        "colab": {}
      },
      "source": [
        "real_data_vals, final_samples_vals, data_reconstructions_vals = sess.run(\n",
        "      [real_data, samples, reconstructions])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UpqpNO87Kkud",
        "colab": {}
      },
      "source": [
        "fig, axes = plt.subplots(1, 3, figsize=(3*4,4))\n",
        "\n",
        "\n",
        "show_digits(axes[0], real_data_vals, 'Data')\n",
        "show_digits(axes[1], data_reconstructions_vals, 'Reconstructions')\n",
        "show_digits(axes[2], final_samples_vals, 'Samples')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw_VtACw8uD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_latent_interpolations(lambda x: decoder(x).mean(), prior, sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HREJ6CfDWRBA",
        "colab_type": "text"
      },
      "source": [
        "## Discussion about constrained optimization\n",
        "\n",
        "* What do you observe about the behaviour of the likelihood and KL term throughout training? How is it different than in Amortized Variational inference with and without KL annealing?\n",
        "* What do you notice about the behaviour of the lagrangian during training?\n",
        "* How would you choose the hyperparameters of the constrained optimization problem?"
      ]
    }
  ]
}